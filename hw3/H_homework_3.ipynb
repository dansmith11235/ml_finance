{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: counting parameters (30%):\n",
    "## Count the parameters for the following three network configurations. \n",
    "## Please show & clearly explain how you arrive at the numbers (not just use count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 512\n",
    "MAX_WORDS = 50000\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network 1\n",
    "word_input = Input(\n",
    "    shape=(MAX_SEQ_LEN,), \n",
    "    dtype='int32'\n",
    ")\n",
    "hidden_state = Embedding(\n",
    "    MAX_WORDS, \n",
    "    EMBEDDING_DIM, \n",
    "    input_length=MAX_SEQ_LEN\n",
    ")(word_input)\n",
    "hidden_state = LSTM(\n",
    "    64, dropout=0.2)(hidden_state)\n",
    "hidden_state = Dense(32)(hidden_state)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "output = Dense(10, activation='softmax')(hidden_state)\n",
    "model = Model(word_input, output)\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [50000 * 50] + [() + ] + [(64 * 32) + 32] + [(32 * 10) + 10]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network 2\n",
    "word_input = Input(\n",
    "    shape=(MAX_SEQ_LEN,), \n",
    "    dtype='int32'\n",
    ")\n",
    "hidden_state = Embedding(\n",
    "    MAX_WORDS, \n",
    "    EMBEDDING_DIM, \n",
    "    input_length=MAX_SEQ_LEN\n",
    ")(word_input)\n",
    "\n",
    "hidden_state = SimpleRNN(\n",
    "    64, \n",
    "    return_sequences=True\n",
    ")(hidden_state)\n",
    "\n",
    "hidden_state = LSTM(\n",
    "    64, \n",
    "    recurrent_dropout=0.2\n",
    ")(hidden_state)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(hidden_state)\n",
    "model = Model(word_input, output)\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network 3\n",
    "word_input = Input(\n",
    "    shape=(MAX_SEQ_LEN,), \n",
    "    dtype='int32'\n",
    ")\n",
    "hidden_state = Embedding(\n",
    "    MAX_WORDS, \n",
    "    EMBEDDING_DIM, \n",
    "    input_length=MAX_SEQ_LEN\n",
    ")(word_input)\n",
    "\n",
    "hidden_state = LSTM(\n",
    "    64, \n",
    "    return_sequences=True\n",
    ")(hidden_state)\n",
    "\n",
    "hidden_state = LSTM(\n",
    "    64, \n",
    "    return_sequences=True\n",
    ")(hidden_state)\n",
    "\n",
    "\n",
    "hidden_state = Bidirectional(LSTM(\n",
    "    64, \n",
    "    recurrent_dropout=0.2\n",
    "))(hidden_state)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(hidden_state)\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Many Ways to Solve the Same Problem (35%)\n",
    "### ... and the limits of deep learning...\n",
    "\n",
    "## We saw three ways to solve the IMDB problem:\n",
    " - flatten the sequence and send to a dense network\n",
    " - a (bidirectional) LSTM to get sequence information\n",
    " - being \"clever\" with the features we use (FastText)\n",
    " \n",
    "\n",
    "# We want to try to combine these \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "np.random.seed(\n",
    "    abs(hash('mit') // (2 ** 32 -1))\n",
    ")\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import (\n",
    "    SimpleRNN, Dense, LSTM,\n",
    "    Embedding, Input, Dropout,\n",
    "    Concatenate\n",
    ")\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d1ccd54a829d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ((int_sequences_train, y_train), \n\u001b[0;32m----> 9\u001b[0;31m  \u001b[0;34m(\u001b[0m\u001b[0mint_sequences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_WORDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mindex_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imdb' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "INDEX_FROM = 3   # word index offset\n",
    "MAX_WORDS = 50000\n",
    "MAX_SEQ_LEN = 500\n",
    "EMBEDDING_DIM = 50\n",
    "NUM_HIDDEN_RNN = 64\n",
    "\n",
    "\n",
    "((int_sequences_train, y_train), \n",
    " (int_sequences_test, y_test)) = imdb.load_data(\n",
    "    num_words=MAX_WORDS, \n",
    "    index_from=INDEX_FROM\n",
    ")\n",
    "\n",
    "# Some word magic\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v + INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "((int_sequences_train, y_train), \n",
    " (int_sequences_test, y_test)) = imdb.load_data(\n",
    "    num_words=MAX_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = '/Users/Hugo/Documents/data/text/glove.6B/glove.6B.50d.txt'  # FIXME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "word_vecs = load_glove_file(GLOVE_FILE)\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for  i, word in id_to_word.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solve the problem with the \"Fasttext\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indices, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indices:\n",
    "                    new_list.append(token_indices[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_RANGE = 2\n",
    "MAX_WORDS_FT = 5000\n",
    "\n",
    "ft_sequences_train = int_sequences_train[:]  # makes a copy\n",
    "ft_sequences_test = int_sequences_test[:]  # makes a copy\n",
    "\n",
    "# TODO(filter ft_sequences to only elements that are less than MAX_WORDS_FT)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "if NGRAM_RANGE > 1:\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in ft_sequences_train:\n",
    "        for i in range(2, NGRAM_RANGE + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = MAX_WORDS_FT + 1\n",
    "    token_indices = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indices_token = {token_indices[k]: k for k in token_indices}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    MAX_FEATURES_FASTTEXT = np.max(list(indices_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    ft_sequences_train = add_ngram(ft_sequences_train, token_indices, NGRAM_RANGE)\n",
    "    ft_sequences_test = add_ngram(ft_sequences_test, token_indices, NGRAM_RANGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pad the sequences\n",
    "# YOUR CODE HERE\n",
    "#ft_sequences_train ... \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import Flatten, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "fastext_word_input = Input(\n",
    "    shape=(MAX_SEQ_LEN,),\n",
    "    dtype='int32'\n",
    ")\n",
    "# your code here, don't forget to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~40s epoch\n",
    "model.fit(\n",
    "    ft_sequences_train,\n",
    "    y_train, \n",
    "    epochs=8, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pd.DataFrame(model.history.history)[['acc', 'val_acc']].plot(\n",
    "    figsize=(16,9), fontsize=16, style='o', markersize=16\n",
    ")\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('epoch', fontsize=16)\n",
    "\n",
    "\n",
    "preds_ft = model.predict(ft_sequences_test).squeeze()\n",
    "test_acc_ft = accuracy_score(y_test, (preds_ft > 0.5).astype(int))\n",
    "plt.title('test accuracy = {:.3f}'.format(test_acc_ft), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solve the problem with an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "single_word_input = Input(shape=(MAX_SEQ_LEN,), \n",
    "                   dtype='int32')\n",
    "\n",
    "# TODO: \n",
    "# make a non-trainable embedding\n",
    "# make a single layer lstm\n",
    "# make a single neuron dense layer\n",
    "\n",
    "# your code here\n",
    "# don't forget to compile the model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~60s / epoch\n",
    "model.fit(\n",
    "    int_sequences_train,\n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history)[['acc', 'val_acc']].plot(\n",
    "    figsize=(16,9), fontsize=16, style='o', markersize=16\n",
    ")\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('epoch', fontsize=16)\n",
    "\n",
    "\n",
    "preds_rnn = model.predict(int_sequences_test).squeeze()\n",
    "test_acc_rnn = accuracy_score(y_test, (preds_rnn > 0.5).astype(int))\n",
    "plt.title('test accuracy = {:.3f}'.format(test_acc_rnn), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Solve the problem by flattening the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "single_word_input = Input(shape=(MAX_SEQ_LEN,), \n",
    "                   dtype='int32')\n",
    "\n",
    "# TODO\n",
    "# make a non-trainable embedding\n",
    "# flatten the sequence\n",
    "# add some dorpout\n",
    "# add a dense layer\n",
    "# add dropout\n",
    "# add a dense layer with one neuron\n",
    "\n",
    "# your code here\n",
    "# don't forget to compile the model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    int_sequences_train,\n",
    "    y_train, \n",
    "    epochs=8, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history)[['acc', 'val_acc']].plot(\n",
    "    figsize=(16,9), fontsize=16, style='o', markersize=16\n",
    ")\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('epoch', fontsize=16)\n",
    "\n",
    "\n",
    "preds_flat = model.predict(int_sequences_test).squeeze()\n",
    "test_acc_flat = accuracy_score(y_test, (preds_flat > 0.5).astype(int))\n",
    "plt.title('test accuracy = {:.3f}'.format(test_acc_flat), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Bagging\n",
    "## Improving with bagging\n",
    "In ML there is a common technique to __bag__ multiple estimators togther. \n",
    "\n",
    "The basic idea is to reduce variance by average together multiple estimators that should have\n",
    "different kinds of errors. \n",
    "\n",
    "NB: A random forest is a bagging estimator\n",
    "\n",
    "### Learning weights\n",
    " - We can learn weights for the various estimators (we have enough data to do this).\n",
    " - We'll do it on the test data, which is technically cheating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# TODO\n",
    "# - concatenate the test-set predictions from each method\n",
    "#      it should be a lenght-of-test-data x 3 vector\n",
    "# - fit a 3-feature logistic regression to the data\n",
    "# - print the test_set accuacy\n",
    "\n",
    "# your code here...\n",
    "\n",
    "# lr = LogisticRegression( ...\n",
    "\n",
    "# your code here...\n",
    "\n",
    "print('new accuracy {:.3f}'.format(accuracy_score(y_test, new_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also just guess hard_coded weights based on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_flat = 1\n",
    "wt_rnn = 2\n",
    "wt_ft = 12\n",
    "\n",
    "total_wt = (wt_flat + wt_ft + wt_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - make a weighted average of all the predictions\n",
    "# - calculate the test accuracy\n",
    "\n",
    "# your code here\n",
    "\n",
    "print('test accuracy = {:.4f}'.format(test_acc_voted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: combine them with neural nets\n",
    "## Our goal is to construct a a neural network with two inputs and one output:\n",
    "### Inputs\n",
    " - Embedded glove words, untrainable, for the RNN and the flattening\n",
    " - The fastext bigrams\n",
    "### Output:\n",
    " - the usual `0/1` for the IMDB task\n",
    " \n",
    "### Newtork: \n",
    " - make a single LSTM on the word vectors\n",
    " - flatten the ORIGINAL word-vector sequence and have it go through a single dense layer\n",
    "    - add dropout appropriately\n",
    " - average the fasttext input over all the words\n",
    "\n",
    "This will make 3 different sets of features. You should\n",
    " - concatenate them\n",
    " - add some dropout\n",
    " - send the merged output to a single-neuron dense layer.\n",
    " \n",
    "# $ \\\\ $\n",
    "# Network\n",
    "```\n",
    "int-input-sequence   fastext-input sequence\n",
    "     |                      |\n",
    " embedding                embedding\n",
    " (non-trainable)          (trainabile)\n",
    " |                           |\n",
    " |\\                          |    \n",
    " | \\                         |\n",
    " |  \\                  Average over words\n",
    " |   \\                       |\n",
    " |    \\                      |\n",
    " |     \\                     |\n",
    " Flatt  LSTM                 |\n",
    " |        |                  |\n",
    " Drop     |                 /\n",
    " |        |                /\n",
    " Dense    |               /\n",
    " |         \\             /\n",
    " Drop       \\           /\n",
    " |           \\         /\n",
    " ######################\n",
    " ####### concat #######\n",
    " ######################\n",
    "          |\n",
    "         Drop\n",
    "          |\n",
    "        Dense(1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# Your code here\n",
    "# don't forget to compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def plot_model_in_notebook(model):\n",
    "    return SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a big model\n",
    "### If you can fit it on your computer (probably needs 16GB RAM) then do so\n",
    "### if not, try one epoch with a very small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~2 min epoch\n",
    "model.fit(\n",
    "    [int_sequences_train, ft_sequences_train],\n",
    "    y_train, \n",
    "    epochs=1, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pd.DataFrame(model.history.history)[['acc', 'val_acc']].plot(\n",
    "    figsize=(16,9), fontsize=16, style='o', markersize=16\n",
    ")\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('epoch', fontsize=16)\n",
    "\n",
    "\n",
    "preds = (model.predict([\n",
    "    int_sequences_test, \n",
    "    ft_sequences_test\n",
    "]) > 0.5).squeeze().astype(int)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "plt.title('test accuracy = {:.3f}'.format(test_acc), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The crux of the problem!\n",
    "## Part 6: Why is this worse than combining by hand? \n",
    "### Please put plain text answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: NER, again! (35%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = '/Users/Hugo/Documents/data/text/glove.6B/glove.6B.50d.txt' # FIXME\n",
    "DATA_PATH = '/Users/Hugo/Documents/Python/mit_classes/analysis_ml/ml_finance/hw2/'  # FIXME\n",
    "MAX_SEQ_LEN = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "def load_dataset(fname):\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\", line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        if cur:\n",
    "            docs.append(cur)\n",
    "    return docs\n",
    "\n",
    "word_vecs = load_glove_file(GLOVE_FILE)\n",
    "docs = load_dataset(os.path.join(DATA_PATH, 'train.conll'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'ORG'],\n",
       " ['rejects', 'O'],\n",
       " ['German', 'MISC'],\n",
       " ['call', 'O'],\n",
       " ['to', 'O'],\n",
       " ['boycott', 'O'],\n",
       " ['British', 'MISC'],\n",
       " ['lamb', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_first_doc = [\n",
    "    ['EU', 'ORG'],\n",
    "    ['rejects', 'O'],\n",
    "    ['German', 'MISC'],\n",
    "    ['call', 'O'],\n",
    "    ['to', 'O'],\n",
    "    ['boycott', 'O'],\n",
    "    ['British', 'MISC'],\n",
    "    ['lamb', 'O'],\n",
    "    ['.', 'O']\n",
    "]\n",
    "\n",
    "assert len(word_vecs) == 400000, 'word vectors did not load properly'\n",
    "assert word_vecs['the'].shape == (50,), 'word vectors did not load properly'\n",
    "assert len(docs) == 14041, 'something has gone wrong with data loading'\n",
    "assert docs[0] == correct_first_doc, 'something has gone wrong with data loading'\n",
    "\n",
    "\n",
    "MAX_WORDS = len(word_vecs)  # max number of words to use in the embedding\n",
    "UNKNOWN = 'UUUNKKK'  # token for unknown word\n",
    "UNKNOWN_WORD_INDEX = 0\n",
    "EMBEDDING_DIM = 50  # dimension of embedding\n",
    "NULL_TAG = 'O'  # tags that are not a named entity\n",
    "\n",
    "# Some derived quantities\n",
    "TAGS = (NULL_TAG, 'LOC', 'PER', 'ORG', 'MISC')\n",
    "NUM_TO_TAG = {num + 1: tag for num, tag in enumerate(TAGS)}\n",
    "TAG_TO_NUM = {tag: num for num, tag in NUM_TO_TAG.items()}\n",
    "\n",
    "NUM_CLASSES = len(TAG_TO_NUM) + 1  # for padding\n",
    "assert NUM_CLASSES == 6, 'somethig has gone wrong'\n",
    "\n",
    "word_to_num = {word: idx + 1 for idx, word in enumerate(word_vecs.keys())}\n",
    "num_to_word = {num: word for word, num in word_to_num.items()}\n",
    "\n",
    "word_to_num[UNKNOWN] = UNKNOWN_WORD_INDEX\n",
    "num_to_word[UNKNOWN_WORD_INDEX] = UNKNOWN\n",
    "\n",
    "assert word_to_num['the'] < 10, '\"the\" is not a common word- something has gone wrong.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_to_num.items():#tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1, 'LOC': 2, 'PER': 3, 'ORG': 4, 'MISC': 5}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_TO_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_sequences(doc, word_to_num, tag_to_num):\n",
    "    \"\"\"Process a sequence\n",
    "    Args:\n",
    "        doc: a sequence of word, tag  (both are str)\n",
    "        word_to_num: a dictionary mapping a word to an integer\n",
    "        tag_to_num: a dictionary mapping a tag to an integer\n",
    "    \"\"\"\n",
    "    int_seq = []\n",
    "    tags = []\n",
    "    # TODO loop through doc and assembed `int_seq`, and `tags`\n",
    "    #     both of which should be integers instead of strings\n",
    "    for p in doc:\n",
    "        \n",
    "        try:\n",
    "            int_seq.append(word_to_num[p[0].lower()])\n",
    "            tags.append(tag_to_num[p[1]])\n",
    "        except:\n",
    "            int_seq.append(0)  # Make 0 if not in dict\n",
    "            tags.append(1)   # tag as MISC if not in dict \n",
    "    return int_seq, tags\n",
    "\n",
    "int_sequences = []\n",
    "tag_seqs = []\n",
    "\n",
    "# TODO\n",
    "# loop through docs\n",
    "# if the length of the document is less than 2, ignore it\n",
    "# otherwise process it into an integer sequence of words and\n",
    "# an integer sequences of tags (targets)\n",
    "# append the result to int_sequences and tag_seqs\n",
    "for doc in docs:\n",
    "    if len(doc) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        int_seq, tags = process_sequences(doc=doc, word_to_num=word_to_num, tag_to_num=TAG_TO_NUM)\n",
    "        int_sequences.append(int_seq)\n",
    "        tag_seqs.append(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tag_seqs:\n",
    "    if len(np.unique(i)) > 5:\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max(map(len, int_sequences)) == 113, 'something has gone wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# TODO:\n",
    "# 1. padd the integer sequences of words\n",
    "# 2. to the same with the tags (targets)\n",
    "# 3. Turn the tags into categorical variables\n",
    "\n",
    "int_sequences = pad_sequences(int_sequences, MAX_SEQ_LEN)\n",
    "tag_seqs = pad_sequences(tag_seqs, MAX_SEQ_LEN)\n",
    "tag_seqs = to_categorical(tag_seqs)\n",
    "\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13862, 115, 6), (13862, 115))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_seqs.shape, int_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (\n",
    "    SimpleRNN, Dense, LSTM,\n",
    "    Embedding, Input, Dropout\n",
    ")\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 115)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 115, 50)           20000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 115, 64)           29440     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 115, 6)            390       \n",
      "=================================================================\n",
      "Total params: 20,029,830\n",
      "Trainable params: 29,830\n",
      "Non-trainable params: 20,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#TODO: Build a netowrk\n",
    "# 1. make an input\n",
    "# 2. make a non-trainable embedding\n",
    "# 3. make a single, LSTM, add dropout and recurrent dropout\n",
    "# 4. Make a single dense hidden layer\n",
    "# 5. Make the correct-sized output layer\n",
    "#       the output should be softmax\n",
    "\n",
    "# make a model and compile it\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "hidden_state = Embedding(trainable=False, weights=[embedding_matrix],\n",
    "            input_dim=MAX_WORDS, \n",
    "            output_dim=EMBEDDING_DIM, \n",
    "            input_length=MAX_SEQ_LEN\n",
    ")(word_input)\n",
    "hidden_state = LSTM(\n",
    "    64, return_sequences=True,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2)(hidden_state)\n",
    "output = TimeDistributed(Dense(6, activation='softmax'))(hidden_state)\n",
    "model = Model(word_input, output)\n",
    "model.compile(\n",
    "    optimizer='rmsprop', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 491.00 304.00\" width=\"491pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 487,-300 487,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 112444680232 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>112444680232</title>\n",
       "<polygon fill=\"none\" points=\"107,-249.5 107,-295.5 376,-295.5 376,-249.5 107,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.5\" y=\"-268.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-249.5 236,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-272.5 292,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"292,-249.5 292,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-280.3\">(None, 115)</text>\n",
       "<polyline fill=\"none\" points=\"292,-272.5 376,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334\" y=\"-257.3\">(None, 115)</text>\n",
       "</g>\n",
       "<!-- 112444679112 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>112444679112</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-166.5 78.5,-212.5 404.5,-212.5 404.5,-166.5 78.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-185.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"243.5,-166.5 243.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"243.5,-189.5 299.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"299.5,-166.5 299.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-197.3\">(None, 115)</text>\n",
       "<polyline fill=\"none\" points=\"299.5,-189.5 404.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-174.3\">(None, 115, 50)</text>\n",
       "</g>\n",
       "<!-- 112444680232&#45;&gt;112444679112 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>112444680232-&gt;112444679112</title>\n",
       "<path d=\"M241.5,-249.3799C241.5,-241.1745 241.5,-231.7679 241.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.0001,-222.784 241.5,-212.784 238.0001,-222.784 245.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112444679056 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>112444679056</title>\n",
       "<polygon fill=\"none\" points=\"111.5,-83.5 111.5,-129.5 371.5,-129.5 371.5,-83.5 111.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-102.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"210.5,-83.5 210.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"210.5,-106.5 266.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"266.5,-83.5 266.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-114.3\">(None, 115, 50)</text>\n",
       "<polyline fill=\"none\" points=\"266.5,-106.5 371.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-91.3\">(None, 115, 64)</text>\n",
       "</g>\n",
       "<!-- 112444679112&#45;&gt;112444679056 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>112444679112-&gt;112444679056</title>\n",
       "<path d=\"M241.5,-166.3799C241.5,-158.1745 241.5,-148.7679 241.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.0001,-139.784 241.5,-129.784 238.0001,-139.784 245.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112389685544 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>112389685544</title>\n",
       "<polygon fill=\"none\" points=\"0,-.5 0,-46.5 483,-46.5 483,-.5 0,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161\" y=\"-19.8\">time_distributed_1(dense_1): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"322,-.5 322,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"322,-23.5 378,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"378,-.5 378,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"430.5\" y=\"-31.3\">(None, 115, 64)</text>\n",
       "<polyline fill=\"none\" points=\"378,-23.5 483,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"430.5\" y=\"-8.3\">(None, 115, 6)</text>\n",
       "</g>\n",
       "<!-- 112444679056&#45;&gt;112389685544 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>112444679056-&gt;112389685544</title>\n",
       "<path d=\"M241.5,-83.3799C241.5,-75.1745 241.5,-65.7679 241.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.0001,-56.784 241.5,-46.784 238.0001,-56.784 245.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def plot_model_in_notebook(model):\n",
    "    return SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11089 samples, validate on 2773 samples\n",
      "Epoch 1/10\n",
      "11089/11089 [==============================] - 12s 1ms/step - loss: 0.3689 - acc: 0.9647 - val_loss: 0.1316 - val_acc: 0.9760\n",
      "Epoch 2/10\n",
      "11089/11089 [==============================] - 11s 1ms/step - loss: 0.0982 - acc: 0.9790 - val_loss: 0.0721 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "11089/11089 [==============================] - 12s 1ms/step - loss: 0.0583 - acc: 0.9862 - val_loss: 0.0509 - val_acc: 0.9871\n",
      "Epoch 4/10\n",
      "11089/11089 [==============================] - 12s 1ms/step - loss: 0.0430 - acc: 0.9892 - val_loss: 0.0403 - val_acc: 0.9891\n",
      "Epoch 5/10\n",
      "11089/11089 [==============================] - 11s 993us/step - loss: 0.0356 - acc: 0.9906 - val_loss: 0.0356 - val_acc: 0.9903\n",
      "Epoch 6/10\n",
      "11089/11089 [==============================] - 12s 1ms/step - loss: 0.0314 - acc: 0.9915 - val_loss: 0.0319 - val_acc: 0.9910\n",
      "Epoch 7/10\n",
      "11089/11089 [==============================] - 11s 962us/step - loss: 0.0291 - acc: 0.9921 - val_loss: 0.0298 - val_acc: 0.9915\n",
      "Epoch 8/10\n",
      "11089/11089 [==============================] - 12s 1ms/step - loss: 0.0273 - acc: 0.9926 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 9/10\n",
      "11089/11089 [==============================] - 11s 1ms/step - loss: 0.0260 - acc: 0.9929 - val_loss: 0.0275 - val_acc: 0.9921\n",
      "Epoch 10/10\n",
      "11089/11089 [==============================] - 11s 1ms/step - loss: 0.0249 - acc: 0.9931 - val_loss: 0.0269 - val_acc: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2fa010b8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    int_sequences, tag_seqs, \n",
    "    validation_split=0.2, \n",
    "    epochs=10, \n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today: O\n",
      "people: O\n",
      "will: O\n",
      "vote: O\n",
      "for: O\n",
      "president: O\n",
      "in: O\n",
      "Fooland: O\n",
      ",: O\n",
      "Janet: PER\n",
      "said: O\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "#sentence = 'I live in Brussels , BG.'.split()\n",
    "sentence = 'Today people will vote for president in Fooland , Janet said .'.split()\n",
    "\n",
    "# TODO:\n",
    "# 1. turn sentence (list of str) into an integer sequence\n",
    "# 2. Use the model, call `predict` on the integer sequence\n",
    "# 3. create a list of predicted labels for each word called `predicted_labels`\n",
    "# 4. remove the padding from predicted_labels\n",
    "\n",
    "sent_int = []\n",
    "for w in sentence:\n",
    "    if w.lower() in word_to_num.keys():\n",
    "        sent_int.append(word_to_num[w.lower()])\n",
    "    else:\n",
    "        sent_int.append(0)  # Make it zero if unmapped\n",
    "        \n",
    "sent_int = pad_sequences([sent_int], MAX_SEQ_LEN)\n",
    "preds = model.predict(sent_int)\n",
    "preds = np.argmax(preds[0], axis=1)\n",
    "\n",
    "predicted_labels = []\n",
    "for t in preds:\n",
    "    if t > 0:\n",
    "        predicted_labels.append(NUM_TO_TAG[t])\n",
    "    \n",
    "for word, lab in zip(sentence, predicted_labels):\n",
    "    print('{}: {}'.format(word, lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1- small data learning with embeddings (30%)\n",
    "### We're going to use pre-trained embeddings to try to learn a text classification problem with few training examples\n",
    "### This is very similar to what we did in class!\n",
    "## $ \\\\ $\n",
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=1234)\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "### a. What is the most common class in the train set?\n",
    "### b. What is the out of sample (test) accuracy if we guess the most probable class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 480,\n",
       "         1: 584,\n",
       "         2: 591,\n",
       "         3: 590,\n",
       "         4: 578,\n",
       "         5: 593,\n",
       "         6: 585,\n",
       "         7: 594,\n",
       "         8: 598,\n",
       "         9: 597,\n",
       "         10: 600,\n",
       "         11: 595,\n",
       "         12: 591,\n",
       "         13: 594,\n",
       "         14: 593,\n",
       "         15: 599,\n",
       "         16: 546,\n",
       "         17: 564,\n",
       "         18: 465,\n",
       "         19: 377})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hint: you can do this in many ways, including collections.Counter or pandas\n",
    "#print('most common class {}: {}'.format(most_common_class, data_train.target_names[most_common_class]))\n",
    "Counter(data_train.target)\n",
    "# Class 10 is the most common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052973977695167283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data_test.target == 10)/data_test.target.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Turn the text into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Instantiate a tokenizer with max workds\n",
    "# 2. fit the tokenizer on text\n",
    "# 3. Turn the text into integer sequences (train and test)\n",
    "# 4. pad the sequences to a constant sequence length (train and test)\n",
    "# 5. turn y into categorical variables\n",
    "\n",
    "t = Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "t.fit_on_texts(data_train.data)\n",
    "\n",
    "s_test = t.texts_to_sequences(data_test['data'])\n",
    "s_train = t.texts_to_sequences(data_train['data'])\n",
    "\n",
    "\n",
    "int_sequences_train = pad_sequences(s_train, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "\n",
    "int_sequences_test = pad_sequences(s_test, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "y_train =to_categorical(data_train['target'])\n",
    "\n",
    "y_test = to_categorical(data_test['target'])\n",
    "\n",
    "\n",
    "# you should have 4 variables:\n",
    "# y_train, y_test, int_sequences_train, int_sequences_test\n",
    "# all are numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.shape ==(11314, 20), 'something went wrong'\n",
    "assert int_sequences_test.shape == (7532, 100), 'something went wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: load the GloVe embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = '/home/dan/MFin/MLfin'  # FIXME directory with glove\n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "word_vecs = load_glove_file(GLOVE_PATH)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = y_train.shape[1]\n",
    "assert NUM_CLASSES == 20, 'something went wrong'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 150)          7650      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 150)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                3020      \n",
      "=================================================================\n",
      "Total params: 510,670\n",
      "Trainable params: 510,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (Input, Embedding, Dropout, Dense,\n",
    "GlobalAveragePooling1D, Flatten)\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Build a model with\n",
    "#  - an embedding\n",
    "#  - some number of dense layers\n",
    "#  - dropout\n",
    "#  - don't forget to use GlobalAveragePooling to average over one dimension\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "e = Embedding(input_dim=MAX_WORDS,output_dim=EMBEDDING_DIM,\n",
    "              input_length=MAX_SEQ_LEN,weights=[embedding_matrix])(word_input)\n",
    "e = Dense(150, activation=\"relu\")(e)\n",
    "e = Dropout(0.3)(e)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(e)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation=\"softmax\")(avg_pool)\n",
    "\n",
    "# Add code here\n",
    "\n",
    "# output = ...\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_hinge', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510670"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_train = 100\n",
    "epochs = 1000  # this is a big number but won't take long with 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2219861922464153"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you should be able to get more than 20% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare to others methods\n",
    "### a. How does this compare to a randomly initialized, trainable embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Build the same model as above, but with a random embedding\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "e = Embedding(input_dim=MAX_WORDS,output_dim=EMBEDDING_DIM,\n",
    "              input_length=MAX_SEQ_LEN,trainable=True)(word_input)\n",
    "e = Dense(150, activation=\"relu\")(e)\n",
    "e = Dropout(0.3)(e)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(e)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation=\"softmax\")(avg_pool)\n",
    "\n",
    "# your code here\n",
    "\n",
    "# output = ...\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11510886882634094"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b: how does this compare to logistic regression trained on 100 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12413701540095592"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "samples_to_train = data_train['data'][:100]\n",
    "samples_to_target = data_train['target'][:100]\n",
    "\n",
    "# TODO\n",
    "# 1. make a count vectorizer\n",
    "# 2. fit it on only `samples_to_train` data points\n",
    "# 3. trainsform train and test data into integers\n",
    "# 4. fit logistic regression on just `num_samples_to_train` samples\n",
    "# 5. Compute accuracy score\n",
    "\n",
    "vec = CountVectorizer().fit(samples_to_train)\n",
    "int_train =vec.transform(samples_to_train)\n",
    "int_test = vec.transform(data_test['data'])\n",
    "lr = LogisticRegression()\n",
    "lr.fit(int_train, samples_to_target)\n",
    "\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(int_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This should be approximately 10-12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Homework problem: improving BOW (30%)\n",
    "There are many improvements that can be made to the bag of words represetation, without resorting to neural networks.\n",
    "Here we'll try one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe to restart notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: fit a bag of words and logistic regression to the 20 newsgroups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "# 1. make a count vectorizer with max_features=20000\n",
    "# 2. fit it\n",
    "# 3. transform the train and test data into number\n",
    "vec = CountVectorizer(max_features=20000)\n",
    "vec.fit(data_train.data)\n",
    "xtr = vec.transform(data_train.data)\n",
    "xte = vec.transform(data_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60647902283590016"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. fit logistic regression\n",
    "# 2. compute accuracy score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtr, data_train.target)   \n",
    "accuracy_score(data_test.target, lr.predict(xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TFIDF\n",
    "A big problem with counting words is that we'll tend to overweight very common words. \n",
    "These common words often carry little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 93969),\n",
       " ('to', 51191),\n",
       " ('of', 45608),\n",
       " ('a', 40042),\n",
       " ('and', 39197),\n",
       " ('is', 28204),\n",
       " ('in', 27756),\n",
       " ('I', 27143),\n",
       " ('that', 25016),\n",
       " ('for', 18066)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def word_iterator():\n",
    "    \"\"\"This iterator yields one word at a time from the train data\"\"\"\n",
    "    for doc in data_train.data:\n",
    "        for word in doc.split():\n",
    "            yield word\n",
    "\n",
    "Counter(word_iterator()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF is a scheme that combats this. \n",
    "## TFIDF = $\\text{Term Frequency Inverse Document Frequency} $\n",
    "# $ \\\\ $\n",
    "\n",
    "# $TFIDF\\left(d, t\\right) \\equiv \\frac{\\text{ Count}\\left(d, t\\right)}{\\text{Doc-Freq}\\left(d, t\\right)} \\equiv \\text{ Count}\\left(d, t\\right)\\,\\left(1 + log\\left( \\frac{N_{docs}}{df_{t}}\\right)\\right)$\n",
    "## Where\n",
    "### $df_{t}$ is the number of documents in which term $t$ appears\n",
    "### $N_{docs}$ is the total number of documents\n",
    "### $\\text{Count}\\left(d,t\\right)$ is the number of times term $t$ appears in document $d$ (the count matrix)\n",
    "\n",
    "# $ \\\\ $ \n",
    "# $ \\\\ $ \n",
    "## Like this, we suppress the weight of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 62 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: write turn the count matrix into a TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_vector(count_matrix):\n",
    "    \"\"\"Get the inverse document frequence vector (shape = num_words)\"\"\"\n",
    "    df = np.array((count_matrix > 0).astype(int).sum(axis=0))\n",
    "    return np.log(count_matrix.shape[0] / (1+df))\n",
    "\n",
    "\n",
    "\n",
    "#TODO(fill in this function)\n",
    "def get_tfidf_matrix(count_matrix):\n",
    "    \"\"\"Turn a count matrix into a tfidf matrix\"\"\"\n",
    "    x = get_idf_vector(xtr)\n",
    "    count_matrix = count_matrix.toarray()\n",
    "    count = 0\n",
    "    result = np.zeros((count_matrix.shape[0],count_matrix.shape[1]))\n",
    "    for i in count_matrix:\n",
    "        result[count] = i * x\n",
    "        count = count + 1\n",
    "        \n",
    "    return(result)    \n",
    "        \n",
    "    \n",
    "    \n",
    "    # TODO\n",
    "#     1. get the idfs with the aboce function\n",
    "#     2. turn it into a numpy array `with .toarray()`\n",
    "#     3. loop the the ROWS of the matrix and transform them\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_transformed = get_tfidf_matrix(xtr)\n",
    "xte_transformed = get_tfidf_matrix(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It worked!\n"
     ]
    }
   ],
   "source": [
    "assert xtr_transformed.shape == xtr.shape, 'something has gone wrong'\n",
    "print('It worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62692511949017526"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(xtr_transformed, data_train.target)\n",
    "accuracy_score(data_test.target, lr.predict(xte_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Do the same with scikitlearn's implmenetation\n",
    "### Happily, sklearn does this for us\n",
    "### And it includes some other nice normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67153478491768459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TODO:\n",
    "# 1. instantiate a TfidfVectorizer with max_features = 20000\n",
    "# 2. fit it on the train data\n",
    "# 3. transform train and test data into matrices\n",
    "# 4. fit logistic regression on the train data\n",
    "# 5. compute the accuracy score on the test data\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(max_features=20000)\n",
    "vec.fit(data_train.data)\n",
    "x_train = vec.transform(data_train.data) \n",
    "x_test = vec.transform(data_test.data)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, data_train.target)\n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tuning the number of words to use\n",
    "## Make a plot of how the vocabulary size (`max_features`) impacts results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 85.75 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "for max_features in (100, 500, 1000, 5000, 10000, 20000, 50000, None):\n",
    "    # TODO:\n",
    "    # 1. instantiate a TfidfVectorizer with max_features = max_features\n",
    "    # 2. fit it on the train data\n",
    "    # 3. transform train and test data into matrices\n",
    "    # 4. fit logistic regression on the train data\n",
    "    # 5. compute the accuracy score on the test data\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    vec = TfidfVectorizer(max_features=max_features)\n",
    "    \n",
    "    vec.fit(data_train.data)\n",
    "    \n",
    "    x_train, x_test = vec.transform(data_train.data), vec.transform(data_test.data)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, data_train.target)\n",
    "\n",
    "\n",
    "    if max_features is None:\n",
    "        num_features = len(vec.get_feature_names())\n",
    "    else:\n",
    "        num_features = max_features\n",
    "    results[num_features] = accuracy_score(data_test.target, lr.predict(x_test)) \n",
    "\n",
    "print('this took {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f7a14d47ac8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAHvCAYAAAA2OzuUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmYZGV99//3t7tnYRZgGGYAYRZGEBgWCcJIghskRuVB\nSIKJZjG4AW5JTDQx/vRBRFzQxC3mp2DipUHymCcEA2okKAhJSKABFWRYZKCHfRnoYbaemd7u5486\nNdTUVHefHmo7Ve/XddXVXefc59S3qw/MZ+751n0ipYQkSZKkYulpdQGSJEmSps8gL0mSJBWQQV6S\nJEkqIIO8JEmSVEAGeUmSJKmADPKSJElSATU9yEfEkoi4PCI2RMTGiLgiIpbmOO78iEgTPLY1o3ZJ\nkiSpXUQz15GPiDnA7cB24CNAAi4E5gDHpJS2THLsQcBBVZvnAlcD30kp/U5DipYkSZLaUF+TX+9s\nYAVwWEppDUBE3AHcB5wLfG6iA1NKjwCPVG6LiDdT+hm+2aiCJUmSpHbU7Bn5a4HZKaWTqrbfAJBS\neuU0z/cj4CjgoJTSaN0KlSRJktpcs3vkjwTurLF9NbByOieKiCXAycBlhnhJkiR1m2a31uwDrK+x\nfRBYMM1z/QGlv4jkbqvZd9990/Lly6f5MpIkSVJ+t91229MppUWNfp1mB/l6+kPgpymlOyYbFBHn\nAOcALF26lFtvvbUZtUmSJKlLRcSDzXidZrfWrKf2zPtEM/U1RcQq4HByzManlC5JKR2fUjp+0aKG\n/8VIkiRJaopmB/nVlPrkq60E7prGec4CRoB/rEdRkiRJUtE0O8hfBZwYESvKGyJiOXBStm9KETET\neBPwg5TSugbUKEmSJLW9Zgf5rwFrgSsj4oyIOB24EngYuLg8KCKWRcRoRJxX4xynUWrFce14SZIk\nda2mBvnszq2nAL8ALgUuAwaAU1JKmyuGBtA7QX1nUVrl5nuNrVaSJElqX01ftSal9BBw5hRj1lIK\n87X2ndGAsiRJkqRCaXZrjSRJkqQ6MMhLkiRJBWSQlyRJkgrIIC9JkiQVkEFekiRJKiCDvCRJklRA\nBnlJkiSpgAzykiRJUgEZ5CVJkqQCMshLkiRJBdTX6gIkSZKkdpNSIiVI2ffjCRKlbaX9zz0vj0nZ\n9mYxyEuS1ETV4aD8B/949qf/pOEg2zeedt6eSjt2el4+fnw8x3krx5fryxFaxquOTZPUNV7j56by\nnFOdt+q1yzVNed6K48ezJ7VrrXHeqtfe9fc3wXlrvPaO92F8kvNWbd/pfZjivM8Fzcne36r3qsa1\nUP0+JBLj4zsfu+v7O9F7uuvvdqfzpupaJzl+kveXqlpyn3eS96EoDPKS1EIpJcbGE6PjiZGxccbG\nEyNjidHxcUbHStvHxsdL28rbK8aWx4yOjTNSNbb8/dh4YqTifKNj49nX5843Ojae/eG3azgohbC8\n4aA0YNeguesf4uXjx2udl12PJ+08IzbpecmC8RThoPw7GN/ltUsF1aplyvNW/CzV74M6WwQEEBHZ\nVwhKGwPoidhlDJXPdzm+clvpXJVjdrxmZOee6LzVx1duqzovO42vrPm5nyN6IOjZ9fiqYyufw84/\nS83zTnj8BO/DROetfn+qxuR5f3d+H4KeeO7YCc+bPQc4+6K6XVaTMshLanspVYTbcgCtEUZ3CsLl\n72sF4epz1Qi3I1VBeLQyFI9VhOLx8ZrbJg/NFUF8vPnprq8n6OsNZvT00Nsb9PX00NdT/oNq53BQ\n/oOp/IfjZOGAbHtPTPWH4nPH98TU4WCn81a9dvUf4pX1ThUOqP4DepJwsNN5q7ZVh4MJz7vL+/Dc\n9p6efOHgufdt5xAy0Xl3CS01frc7n3fnY3d+H6qOr3XenX7Wqusnz3krfgc1v686vqfGsezycz/3\n+2CXc9UI3FOdt+r4yv9upLKzm/Q6BnmpA5RnFCtnaUemETBHxscZmyBojoxXhOKKcDtxqK4Ky2M7\nh+mx8uxx1UxxudYdNY8/V2ezs24EzOjpoa836O0JZvSWgm4pAPfsCMJ92Zjy9pl9PcypGDujt4fe\nqtA8oyfo7elhRm9k5+9hxi7nrfE65W3l/dk5eiteZ0Y2dkfN1efqKY03dEhSZzDIq+uMjo2zdWQs\nX7itCMI7Bc1aM60ThduqmeKxGuG28lwTtkFUb6uof2Ss+bO6vVVhtRwqqwNmZVjt7Qn2mNFL3+y+\nHWF0yuBaDsLlULzLa/ZUBdidg+vOgbunRqgujd9Rc0/PjtlRSZLamUFeHWvr8Bj3r9vM/es2c9+T\nm1nz1Gbue2oTDz4z1LR2hhk7hdKqYNrTsyNEVm+bNaNv17DaUzGDO9FMccVsba0Z43Jw3WmmuPw6\nU4bqnc/nrK4kSa1lkFfhbdo2koX0zdyffV3z1GYeXj+048NlvT3BsoVzOHTxPF571P4smDOz1GbQ\nO0FbwwRBuDLQ7tLWUNWK0eusriRJaiCDvApjcMsw9z25iTXZDHt5pv2Jjdt2jJnZ28OKRXM55qC9\nOPO4gzhk8TwO3W8eyxfOZWaf9z+TJEmdwyCvtpJS4qlN27nvyVIbTOVM+zNbhneMmzOzl0MWz+NX\nDllYCuuL53PI4nksWbAHfb0GdkmS1PkM8mqJ8fHEo89u3dG3ft+Tm1mzbjNrntzMpu2jO8btObuP\nQ/ebz6tX7schi+dlM+zzOWDP2X4gUZIkdTWDvBpqdGycBweHsg+bVsywr9vMtpHxHeP2nTeLQxfP\n4zd+6UAO3W/ejtC+aN4sP1QpSZJUg0FedbFtZIyBp7fsCOrl0D7w9JadlkY8cO89eOHieZy4otwS\nUwrse8+Z2cLqJUmSiscgr2nZsn30ueUcdyzruImHBod23LSnJ2DpPnM4ZPF8Tjl8vx1h/YWL5zFv\nlpecJElSPZiqVNOGoZGdPmy6Jns8+uzWHWNm9AbLF85l5Qv25PRjD9wxw37wvnOZPaO3hdVLkiR1\nPoN8F0sp8fTm4R2Bfc1Tm7PVYjbz9ObtO8bNntHDCxfN4/jlC/jdxUs4JFshZtnCOcxwhRhJkqSW\nMMh3kTseeZb+gcHnQvtTm9mwdWTH/vmz+njh4nmcfNiiHeuvH7p4PgfuvYcrxEiSJLUZg3wXeGDd\nZi66+h7+ffWTAOwzdyaHLJ7H/zrmgB3964cuns9+e7pCjCRJUlEY5DvYM5u388Vr7+Mfb36IWX09\nfODXX8SbVi1l33mzWl2aJEmSnieDfAfaOjzG128c4CvX38/WkTF+d9US/uRXX8Si+QZ4SZKkTmGQ\n7yBj44nv/PRR/vqae3l8wzZevXI/Pvjawzlk8bxWlyZJkqQ6M8h3iP+8bx2f/Ld7uPvxjbz4oL34\nwhuP5aUrFra6LEmSJDWIQb7g7nliI5/6t3u44RfrOGjBHnzpd3+J044+wFVmJEmSOpxBvqCe2LCN\nz/3wXi6/7RHmzerjw6cewR/+yjJm9XkjJkmSpG5gkC+YzdtHufiG+/nafz7A+Di87aSDee8ph7D3\nnJmtLk2SJElNZJAviNGxcb59y8N84Ue/4OnNw5x2zAH8xWsOZ+nCOa0uTZIkSS1gkG9zKSV+dPdT\nfPoHd3P/ui2sWr4Pf3fWERy7ZO9WlyZJkqQWMsi3sTseeZZPfP9ubh4YZMW+c7nkzS/h1Sv38+6r\nkiRJMsi3o4cHh/jsv9/LVbc/xsK5M/n4GUfyplVLmdHb0+rSJEmS1CYM8m1kw9AIf3v9Gr5x41oi\n4L0nH8K5r1zB/NkzWl2aJEmS2oxBvg1sHx3j0v95kL+5bg0bt41w5nEH8f5ffxEH7LVHq0uTJElS\nmzLIt1BKie///HEuuvoeHh7cyssP3ZcPve4IVr5gz1aXJkmSpDZnkG+RW9YO8onv383PHn6Ww/ef\nzzfftopXvmhRq8uSJElSQRjkm+z+dZu56Af3cM1dT7LfnrP4zJnHcOZLDqK3x5VoJEmSlJ9Bvkme\n2bydL157H5fd/BCz+3p4/6tfxNtffjBzZvorkCRJ0vSZIpvkD7/ezz1PbOJNJyzhfb/2IhbNn9Xq\nkiRJklRgBvkmeHLjNlY/tpG/fN3hvPOVL2x1OZIkSeoA3mGoCfoHBgH45RULW1yJJEmSOoVBvgn6\nBwaZM7OXI11WUpIkSXVikG+C/oFBXrJsAX29vt2SJEmqD5Nlg63fMsy9T27ipQfv0+pSJEmS1EEM\n8g12y9pSf/yqg+2PlyRJUv0Y5Busf2CQmX09HHPQXq0uRZIkSR3EIN9g/WsHOXbJ3sye0dvqUiRJ\nktRBDPINtHn7KHc+usH+eEmSJNWdQb6BbntwPeMJVhnkJUmSVGcG+QbqH3iG3p7guKULWl2KJEmS\nOoxBvoH6BwY56sC9mDurr9WlSJIkqcMY5Btk28gYtz9sf7wkSZIawyDfID97+FmGx8ZZtdwgL0mS\npPozyDdI/8AgEXCCQV6SJEkNYJBvkP6BQQ7bbz57zZnR6lIkSZLUgQzyDTAyNs5tD663P16SJEkN\nY5BvgDsf3cDWkTFWHbyw1aVIkiSpQxnkG6B/YBCAEw52/XhJkiQ1hkG+AfoHBlmx71wWz5/d6lIk\nSZLUoQzydTY2nuhfO8gq++MlSZLUQAb5Orv3iU1s2jZqkJckSVJDGeTrrH/gGQCDvCRJkhrKIF9n\n/WsHOXDvPThowZxWlyJJkqQOZpCvo5QS/QP2x0uSJKnxmh7kI2JJRFweERsiYmNEXBERS6dx/BER\n8c8R8XREbI2IeyPiTxpZc14PPL2FpzcPG+QlSZLUcH3NfLGImANcB2wHzgIScCHw44g4JqW0ZYrj\nj8+Ovx54B7ABOBSY18CycyuvH2+QlyRJUqM1NcgDZwMrgMNSSmsAIuIO4D7gXOBzEx0YET3APwDX\nppR+s2LXjxtX7vT0Dwyy77yZrNh3bqtLkSRJUodrdmvN6cBN5RAPkFIaAG4Ezpji2FcBRzBJ2G+1\ncn98RLS6FEmSJHW4Zgf5I4E7a2xfDayc4tiXZV9nR8RNETESEU9FxJciYo+6VrkbHlk/xKPPbmXV\ncttqJEmS1HjNDvL7AOtrbB8EFkxx7Auyr/8EXAO8GvgMpV75f6xXgbvruf74hS2uRJIkSd2g2T3y\nz0f5Lx3fSimdl31/fUT0Ap+OiCNSSndXHxQR5wDnACxdmntxnGnrHxhkz9l9HLb//Ia9hiRJklTW\n7Bn59dSeeZ9opr7SM9nXH1Ztvyb7emytg1JKl6SUjk8pHb9o0aLchU5X/8AgJyzfh94e++MlSZLU\neM0O8qsp9clXWwnclePYtvTUpm088PQWl52UJElS0zQ7yF8FnBgRK8obImI5cFK2bzI/oLT+/Guq\ntr82+3pLfUqcvlsGSv+YYJCXJElSszQ7yH8NWAtcGRFnRMTpwJXAw8DF5UERsSwiRiOi3AtPSukZ\n4FPAOyPikxHxaxHxl8B5wDcrl7Rstv6BZ9hjRi9HHbhXq0qQJElSl2nqh11TSlsi4hTg88ClQADX\nAu9LKW2uGBpAL7v+ReMCYBPwbuADwOPAZ4GPN7j0Sd08MMhLli1gRm+z/14kSZKkbtX0VWtSSg8B\nZ04xZi2lMF+9PVG6IVTb3BTq2aFh7n1yE6cefUCrS5EkSVIXcQr5ebp17XpSsj9ekiRJzWWQf576\n1w4ys7eHY5fs3epSJEmS1EUM8s/TzQODvHjJXsye0dvqUiRJktRFDPLPw5bto9z56AbbaiRJktR0\nBvnn4ScPrWdsPLHq4IWtLkWSJEldxiD/PPQPDNIT8JJlC1pdiiRJkrqMQf55uHlgkKMO3It5s5q+\niqckSZK6nEF+N20bGeNnDz/LquX2x0uSJKn5DPK76Y5HNjA8Ou4HXSVJktQSBvnd1D/wDAAnOCMv\nSZKkFjDI76abBwY5bL/5LJg7s9WlSJIkqQsZ5HfD6Ng4tz243rYaSZIktYxBfjesfmwjQ8NjBnlJ\nkiS1jEF+N/QPDAIY5CVJktQyBvndcPPAIMsXzmG/PWe3uhRJkiR1KYP8NI2PJ25ZO+hsvCRJklrK\nID9Nv3hqExu2jrDq4IWtLkWSJEldzCA/TeX++Jc6Iy9JkqQWMshP080Dgxyw12wOWrBHq0uRJElS\nFzPIT0NKif6BUn98RLS6HEmSJHUxg/w0rH1miHWbtvtBV0mSJLWcQX4a+geeAeyPlyRJUusZ5Kfh\n5oFB9pk7kxcumtfqUiRJktTlDPLTcMvaQVYttz9ekiRJrWeQz+mxZ7fy8OBW++MlSZLUFgzyOd2y\ntrR+vEFekiRJ7cAgn9PNA4PMn9XHEQfs2epSJEmSJIN8Xv0Dgxy/fAG9PfbHS5IkqfUM8jk8vXk7\na57azKqDF7a6FEmSJAkwyOdyq/3xkiRJajMG+RxuHhhk9owejj5wr1aXIkmSJAEG+Vz6BwY5bukC\nZvb5dkmSJKk9mEynsHHbCHc9vtG2GkmSJLWVXEE+Il7U6ELa1W1r15OS/fGSJElqL3ln5O+JiGsj\n4rcjoq+hFbWZmwcGmdEb/NKSBa0uRZIkSdohb5B/G7AH8E/AIxHxyYg4uHFltY/+gWc45qC92WNm\nb6tLkSRJknbIFeRTSt9IKf0KcCzwL8C7gfsi4uqIOCMiOrLXfuvwGHc8ssG2GkmSJLWdaQXwlNId\nKaX3AC8AzgX2A64AHoqI8yNivwbU2DI/fWg9o+PJIC9JkqS2s7sz6cuBY7Kvw8CdwJ8BayLiN+tS\nWRu4eWCQnoCXLLM/XpIkSe0ld5CPiJkR8fsR8R/Az4HXA58GlqSUXgssA64GPteQSlugf2CQlS/Y\nkz1nz2h1KZIkSdJO8i4/+dfAo8A3gU3A6cALU0oXpZSeBkgprQe+SCnQF97w6Dg/eWg9q5YvbHUp\nkiRJ0i7yLiX5ZuDrwFdTSgOTjLsHeOvzrqoN/PzRZ9k+Om5/vCRJktpS3iB/UEppeKpB2ez8N59f\nSe3h5oFBAE5Ybn+8JEmS2k/eHvnjIuJ3au3IbhL10jrW1Bb6BwY5dPE8Fs6b1epSJEmSpF3kDfKf\nBo6cYN8RwKfqU057GBtP3Lp2vW01kiRJalt5g/wxwE0T7OvP9neMux/fyObtowZ5SZIkta28QX72\nJGN7gbn1Kac9lPvjDfKSJElqV3mD/N2Ulpys5XTg3vqU0x76B55h6T5zOGCvPVpdiiRJklRT3lVr\nvgpcHBEbga8BjwAHAucAbwfe3Zjymi+lRP/AIL96xH6tLkWSJEmaUK4gn1L6WkQcBvwp8GeVu4DP\np5QuaURxrbDmqc2sHxqxrUaSJEltLe+MPCmlD0TEV4BfAxYCTwM/Sik90KjiWmFHf/xyg7wkSZLa\nV+4gD5BSuh+4v0G1tIX+gUEWz5/FsoVzWl2KJEmSNKFpBXmAiFhMaRWbnaSUHqpLRS1U7o9fdfA+\nRESry5EkSZImlCvIR0QPcCFwLrD3BMN661VUqzw8uJUnNm7jpfbHS5Ikqc3lXX7yfcB7gL8GAvgk\npWA/QKnV5uyGVNdkNw88A8Cqgxe2uBJJkiRpcnmD/FuBC4CLsuffSSl9FDgCeBRY2oDamq5/YJC9\n58zg0MXzWl2KJEmSNKm8QX4FcGtKaQwYBfYASCmNAF8A3taY8pqrf+0gJyzfh54e++MlSZLU3vIG\n+Q3A3Oz7x4DDKvb1AYVvKn9iwzYefGbI/nhJkiQVQt5Va34KrAT+Dfh34GMRsZXS7PwngJ80przm\n6V+brR9vkJckSVIB5A3yX6DUXgPwUeA44LLs+YPAe+tcV9P1DzzD3Jm9rDxgz1aXIkmSJE0pV5BP\nKf2w4vsnImIV8EJgDnB31itfaP0Dg7xk+T709ebtNpIkSZJaZ8rUGhEzI+I7EfGK8rZUsialdEcn\nhPjBLcP84snN9sdLkiSpMKYM8imlYeDX8owtqlvsj5ckSVLB5A3nNwInNrKQVuofGGRmXw/HHLRX\nq0uRJEmScsn7Ydf3A/8aEZuBfwUeB1LlgJTSeJ1ra5r+gUF+acnezOrrbXUpkiRJUi55Z+R/TunD\nrV+ktErNMDBS8RhuSHVNsGnbCKsf22B/vCRJkgol74z8BVTNwHeK2x5cz3iCVQcvbHUpkiRJUm55\nl588v8F1tEz/wCB9PcFxy/ZudSmSJElSbh27Ek1e/QODHHXgXsyZmfcfJyRJkqTWy5VeI+K8KYak\nlNLH61BPU20bGeP2R57lbScd3OpSJEmSpGnJOw19/iT7yr3zhQvydz2+kZGxxEuWLWh1KZIkSdK0\n5GqtSSn1VD+AfYG3AHcCh+R9wYhYEhGXR8SGiNgYEVdExNKcx6YJHsfmff1KG7aWbkq77/xZu3O4\nJEmS1DK73RieUhoE/iEiFgJ/C5w61TERMQe4DtgOnEVpNv9C4McRcUxKaUuOl/4GcHHVtl9Mo/Qd\ntg6PATBnpuvHS5IkqVjq8QnP28nfVnM2sAI4LKW0BiAi7gDuA84FPpfjHI+mlG7anUKrDZWD/Aw/\n6CpJkqRiqceqNacB63KOPR24qRziAVJKA8CNwBl1qGVahoZHAZgzyxl5SZIkFUveVWu+XmPzTOAo\n4Gjgozlf70jgyhrbVwO/nfMc74qIPwfGgJuAj6aU/jPnsTsZsrVGkiRJBZW3p+QUdr2z6zbgQeAL\nwDdznmcfYH2N7YNAnqVjvgV8D3gMWAb8OXBdRLw6pXR9zhp2KAf52X0GeUmSJBVL3ju7Lm9wHbmk\nlN5c8fQ/I+JKSqvmfBx4ea1jIuIc4ByApUt3XhxnaPsoc2b20tMTjSlYkiRJapBm39l1PbVn3iea\nqZ9USmkT8H3ghEnGXJJSOj6ldPyiRYt22jc0MmZbjSRJkgopV5CPiA9GxN9MsO9LWc96Hqsp9clX\nWwnclfMcdbN1eIw9DPKSJEkqoLwz8m8F7phg38+y/XlcBZwYESvKGyJiOXBStm9aImJPSqvm9E/3\nWIAt20ddelKSJEmFlDfIL6W01nstD1D64GkeXwPWAldGxBkRcTqlVWwepuImTxGxLCJGI+K8im0f\niIivRsQbI+JVEXEWpWUr9wc+nPP1d7J1ZMylJyVJklRIeaejh4ADJ9h3EKU7tU4ppbQlIk4BPg9c\nCgRwLfC+lNLmiqEB9LLzXzTuBX4TeAOwF7CRUpB/e0ppt2bkh4btkZckSVIx5Q3y/wn8eURcnlLa\nEdojYhbw/mx/Limlh4AzpxizllKYr9z2XeC7eV8nj6HhMRbMmVnPU0qSJElNkTfInw/8N/CLiPgW\n8CilGfo/ABYCb2lEcY02NDzKXFtrJEmSVEB515G/PSJOBv4K+CCllpdx4L+AM1NKtzeuxMaxtUaS\nJElFlXvJlqwP/RURsQelteDXp5S2NqyyJtg6PMYerlojSZKkAsqVYiNiBjAzpbQlC+9bK/bNBYZT\nSiMNqrEhUkpssbVGkiRJBZV3OvrvgBnA79XYdzEwDLytXkU1w/bRcVLCG0JJkiSpkPKuI38ypfXe\na7kK+NX6lNM8Q8NjAMyZYZCXJElS8eQN8ouBpybYtw7Yrz7lNM+W7aMAzJllj7wkSZKKJ2+Qfwo4\neoJ9RwPP1Kec5tk6ks3I21ojSZKkAsob5L8H/O+IOKZyY0QcDXyYOt+oqRl2tNYY5CVJklRAeftK\nzgNeDdwWEbcAj1C6IdQqYAD4SGPKa5yhcmvNTFtrJEmSVDy5ZuRTSk8DJwCfAgI4Nvv6CeCEbH+h\nOCMvSZKkIpvODaGepTQzf17jymmeIXvkJUmSVGB5e+Q7ztZhW2skSZJUXLlTbEQcCbwDOAyYXbU7\npZQKtZb8lu3OyEuSJKm4cgX5iHgpcAOwFjgUuANYACyl9MHXNQ2qr2HKy096Z1dJkiQVUd7Wmk8C\nVwBHUvqQ69tTSsuBXwN6gQsbUl0DDQ2P0tsTzOzt2u4iSZIkFVjeFHsM8C0gZc97AVJK11EK8Z+q\nf2mNtWX7GHNm9hIRrS5FkiRJmra8QX4msCWlNA4MAgdU7LsXOKrehTXa1uEx++MlSZJUWHmD/BpK\n/fBQ6o9/W0T0REQP8FbgiUYU10hDI2OuWCNJkqTCyptkvwu8AriUUr/894GNwBgwD/jjhlTXQEPb\nR52RlyRJUmHlCvIppfMrvv9RRJwInAnMAa5OKV3TmPIaZ8jWGkmSJBXYbvWWpJR+Cvy0zrU01dDI\nGHvtMaPVZUiSJEm7pWvXXtw6PMpcZ+QlSZJUUF0b5LdsH/NmUJIkSSqsrg3yW0fskZckSVJxdW2Q\nHxoeZa7LT0qSJKmgujLIj40nto2M21ojSZKkwprWlHRE7AucCCwEvptSGoyI2cBwdtfXQtg6MgZg\na40kSZIKK9eMfJR8FngEuAr4OrA8230l8OGGVNcgQ8OjAN7ZVZIkSYWVt7XmQ8B7gQuAlwJRse+7\nwGl1rquhhrY7Iy9JkqRiyzsl/Q7ggpTSpyKiOv2uAV5Y37Iaa2jYIC9JkqRiyzsjfyBw0wT7hoG5\n9SmnObaO2FojSZKkYssb5B8Fjppg34uBgfqU0xzOyEuSJKno8gb5fwbOi4iTKraliHgR8H7g23Wv\nrIG2ZD3yLj8pSZKkosob5M8H7gH+A7gv2/bPwM+z55+ue2UNZGuNJEmSii5Xkk0pbY2IVwG/B7yG\n0gdcnwE+DlyWUhptWIUNUG6tmeuMvCRJkgoq95R0SmkMuDR7FNqQrTWSJEkquLytNR3luQ+72loj\nSZKkYpowyUbEAJByniellAqzlvzQyCiz+nro7YmpB0uSJEltaLIp6RvIH+QLZWj7mEtPSpIkqdAm\nDPIppbc0sY6mGhoes61GkiRJhdaVPfJbR0adkZckSVKh5Q7yEXFoRHwzIn4REVuyr9+IiEMaWWAj\nlGbkDfKSJEkqrlz9Jdka8v8GbAW+DzwJ7Ae8HnhjRLw2pXRDo4qst6HtYy49KUmSpELL2yj+18BP\ngdeklDaXN0bEfOCabP/x9S+vMYZGRtlv/uxWlyFJkiTttrytNSuBiypDPEBKaRNwEXBkvQtrpKFh\nZ+QlSZJUbHmD/CPAzAn2zQQerU85zeHyk5IkSSq6vEH+IuBjEfGCyo0RcSDwUeCT9S6skYaGR11+\nUpIkSYXKnkyRAAAfEklEQVSWN82+EtgTeCAibuK5D7uemH3/quwDsVC6y+tZ9S60nraOOCMvSZKk\nYssb5F8GjAKPA8uyB9lzgJdXjG3ru8EOj44zMpYM8pIkSSq0XEE+pXRwowtplq3DYwC21kiSJKnQ\nuu7OrkMjowDOyEuSJKnQpjUtHRFLgCXALouwp5Suq1dRjTSUzci7/KQkSZKKLO+dXVcAlwGrypuy\nryn7PgGFSMZD222tkSRJUvHlTbN/BywF3gfcAww3rKIGGxoutdbMdUZekiRJBZY3yJ8AvCWl9C+N\nLKYZhkZsrZEkSVLxTefOroWdha9ka40kSZI6Qd4g/0nggxExt5HFNEO5tcZVayRJklRkedeRvzQi\nDgfWZnd2Xb/rkPa+m2vZ1pHyjLxBXpIkScWVd9WatwAfAsaA49i1zaat7+ZaaYutNZIkSeoAedPs\nx4DvAG9PKT3bwHoabuvwKBEwe0bX3QtLkiRJHSRvml0I/P9FD/EA20bHmdXXQ0RMPViSJElqU3mD\n/H8BRzSykGYZHUvM6HE2XpIkScWWt7XmT4D/GxHrgavZ9cOupJTG61lYo4ynRG+vs/GSJEkqtrxB\n/u7s6z9MsD9N41wtNTo+Tl+PQV6SJEnFljd8X0CBVqaZzNh4otcgL0mSpILLu478+Q2uo2lGxxK9\nftBVkiRJBdd1n/ocG7dHXpIkScWXu689ImYCrwMOA2ZX7U4ppY/Xs7BGGUuJPletkSRJUsHlvbPr\nCygtQbmcUq98eUq7sm++EEF+1B55SZIkdYC8U9OfBdYBSymF+JcCK4BPAGuy7wthbCy5ao0kSZIK\nL2+Qfznw18Bj2fPxlNLalNJ5wOXAl/K+YEQsiYjLI2JDRGyMiCsiYun0yoaI+MuISBHxX9M5bnQ8\n0eOHXSVJklRweYP8QuDx7KZPW4AFFfuuA16V5yQRMScbfzhwFvBm4FDgxxExN2ctRMQK4CPAU3mP\nKRsbH6fPD7tKkiSp4PJ+2PURYHH2/f3ArwM/yp6vArblPM/ZlNpwDksprQGIiDuA+4Bzgc/lPM9X\ngMsoffB2WjeiGkvYIy9JkqTCyzsj/2PgFdn3FwMfiIhrIuL7lD7kennO85wO3FQO8QAppQHgRuCM\nPCeIiN8DjgM+lPM1dzLmnV0lSZLUAfLOZn8E2AcgpfSViOgD3gjMAT5D6c6veRwJXFlj+2rgt6c6\nOCIWAJ8H/iKlNBi70es+OuaqNZIkSSq+vHd2fRp4uuL53wB/sxuvtw+wvsb2QXbuu5/IZ4FfAN/Y\njdcGSjeEmjXDdeQlSZJUbLudaCNiZUScma0x33AR8XLgD4F3pZTSVOMrjjsnIm6NiFvXrVuXrSNv\nkJckSVKx5Uq0EfHliPhqxfPfAu4A/hm4KyJOyPl666k98z7RTH2li4G/Bx6JiL0jYm9K/6LQmz2f\nVeuglNIlKaXjU0rHL1q0iPHkOvKSJEkqvrxT068D/rvi+ceA7wIvBvqBj+Y8z2pKffLVVgJ3TXHs\nEcA7KQX+8uMk4MTs+3flKcAeeUmSJHWCvB92PQBYCxARB1EK429PKf08Ir5EaaY8j6uAv4qIFSml\nB7LzLacUyP9yimNPrrHtC0Av8EeU7jA7pbHxRK83hJIkSVLB5Q3yQ8C87PtXAhuBW7Pnm4H5Oc/z\nNeC9wJUR8REgUVq+8mFKrTMARMQySuvVX5BSugAgpXR99cki4lmgr9a+iYyOj9PrDaEkSZJUcHlb\na34CvCcijgLeA/wwu8srwMHA43lOklLaApxCaeWZSynd1GkAOCWltLliaFCaaa/7p1LHxu2RlyRJ\nUvHlnZH/MHA1cDvwLKVe9bLfoNQnn0tK6SHgzCnGrKUU5qc616vyvm7ZWLJHXpIkScWXdx35WyJi\nKXA4cF9KaWPF7kuA+xpRXCOMjTkjL0mSpOLLOyNfbou5rcb279e1ogYrrSNvkJckSVKxdd2dkcYM\n8pIkSeoAXRfkR8cTfd7ZVZIkSQXXdYl23Bl5SZIkdYAJg3xE7BnReXdOGnX5SUmSJHWAyWbk1wMn\nAETEdRFxeHNKaqyx8USPQV6SJEkFN1mQHwZmZN+/Ctiz4dU0wej4uDPykiRJKrzJlp+8D/j/IuKf\ns+enTjYrn1L6h7pW1iDjCXvkJUmSVHiTBfkPA98CXgck4LxJxiag7YN8yr46Iy9JkqSimzDIp5S+\nGxH7AAcBA8AbgNubVVhDZEm+1+UnJUmSVHCT3tk1pTQGPBgRHwNuSik91pyyGiNlSb7XHC9JkqSC\nmzTIl6WUPgaQLUe5EtgHGATuSimlyY5tJ8kZeUmSJHWI3Ik2It4BPA7cAVyffX0sIt7emNLqzx55\nSZIkdYpcM/IR8fvAJcC1lD4A+wSwP/D7wCURMZRS+j8Nq7LOXLVGkiRJRZcryAN/AVyWUnpz1fZv\nRsSlwAeBtg/y5dYaZ+QlSZJUdHlbaw6jNBNfy7ey/W2v/GFX7+wqSZKkossb5DdRWoayloOy/e3P\nGXlJkiR1iLxB/gfAJyPi5ZUbI+KXgQuz/W2v/GFXe+QlSZJUdNPpkT8RuD4iHqW0es3+lGbj12T7\n29+OGXmXn5QkSVKx5V1H/omIOBZ4G/BySuvIrwVuAL6RUhpqWIV19NwNoZyRlyRJUrHlnZEnC+tf\nzh6FZGuNJEmSOkVX9Zi4/KQkSZI6RVcFeWytkSRJUofoqiDvjLwkSZI6RXcF+eyrM/KSJEkquq4K\n8mUGeUmSJBVdriAfEQ9ExIsn2HdURDxQ37Iao9xaY5CXJElS0eWdkV8OzJpg32xgWV2qabDyOvLe\nEEqSJElFN51EmybYfjzwbB1qaTxn5CVJktQhJrwhVET8KfCn2dMEfDcihquG7UHpLq/fbkx59VX+\nm0hfr0FekiRJxTbZnV0fAK7Nvj8LuBVYVzVmO3AX8Hf1L63+ykG+JwzykiRJKrYJg3xK6UrgSoAo\nBd8LUkoDTaqrMVK5R94gL0mSpGKbbEZ+h5TSWxtdSDO4jrwkSZI6Ra4gHxHnTTEkpZQ+Xod6Gqt8\nZ1d75CVJklRwuYI8cP4k+8oT3W0f5J2RlyRJUqfItfxkSqmn+gHsC7wFuBM4pIE11s2OIO+HXSVJ\nklRweWfkd5FSGgT+ISIWAn8LnFq3qhokJW8IJUmSpM5Qj0R7O/CKOpynaXrtkZckSVLB1SPIn8au\n68u3pVT+sKs98pIkSSq4vKvWfL3G5pnAUcDRwEfrWVSjJCDww66SJEkqvrw98qfw3GdFy7YBDwJf\nAL5Zz6IaJvsJ/LCrJEmSii7vDaGWN7iOpkgkIqDHGXlJkiQVXFct35KwP16SJEmdIXeQj4gDIuKv\nIuKWiLg/+/qZiNi/kQXWVbI/XpIkSZ0hV5CPiBcBPwP+GNgM9Gdf/wT4WUQc2rAK66g0I99V/wgh\nSZKkDpX3w64XARuBl6aU1pY3RsQy4Jps/2/Vvbo6SynhhLwkSZI6Qd7p6ZOB/10Z4gFSSg8C52f7\nC6Gv1xl5SZIkFV/eVDsT2DTBvk3Z/raXsEdekiRJnSFvkP8Z8EcRsdP4iAjg3dn+tpeSq9ZIkiSp\nM+Ttkb8A+B5wd0T8E/A4sD/w28ChwP9qTHn154y8JEmSOkHeG0JdHRGnARcCHwaCUqfKbcBpKaVr\nGldi/aSUDPKSJEnqCHln5EkpXQ1cHRFzgAXA+pTSUMMqawB75CVJktQpcgf5siy8FyrAV7JHXpIk\nSZ2gq9ZiTAl6vSGUJEmSOkBXpdpEckZekiRJHaGrgjwJegzykiRJ6gBdFeQT9shLkiSpM3RXkE+u\nWiNJkqTO0FVBHpyRlyRJUmfoqiCf8IZQkiRJ6gzdFeRtrZEkSVKH6K4gj601kiRJ6gxdFeRxRl6S\nJEkdoquCfOmGUF31I0uSJKlDdV2qdUZekiRJnaCrgrwfdpUkSVKn6K4gj0FekiRJnaG7gnxKrloj\nSZKkjtBVQR6ckZckSVJn6Kog7zrykiRJ6hRND/IRsSQiLo+IDRGxMSKuiIilOY5bFhFXRsSDEbE1\nIp6OiBsi4tS8r50S9BjkJUmS1AGaGuQjYg5wHXA4cBbwZuBQ4McRMXeKw+cBTwMfAU4F3g5sAr4f\nEb+VtwZn5CVJktQJ+pr8emcDK4DDUkprACLiDuA+4FzgcxMdmFJaTSm87xAR3wcGgLcCV0z14ikl\ner0hlCRJkjpAs1Pt6cBN5RAPkFIaAG4EzpjuyVJKo8AGYDTXeJyRlyRJUmdodpA/ErizxvbVwMo8\nJ4iInojoi4j9I+I84EXAl/MWYI+8JEmSOkGzW2v2AdbX2D4ILMh5js8A78++3wy8KaV0bd4CnJGX\nJElSJyhiw/gXgBOA1wM/AP4xIk6baHBEnBMRt0bEreA68pIkSeoMzQ7y66k98z7RTP0uUkqPpJRu\nTSl9L6X0O8BNwF9NMv6SlNLxKaXjwRl5SZIkdYZmB/nVlPrkq60E7trNc94KHJJ3sDPykiRJ6gTN\nDvJXASdGxIryhohYDpyU7ZuWiOgBXgbcn/cYg7wkSZI6QbM/7Po14L3AlRHxEUorQn4ceBi4uDwo\nIpZRCucXpJQuyLadT6kF50bgCWB/SuvKrwJ+L28BttZIkiSpEzQ1yKeUtkTEKcDngUuBAK4F3pdS\n2lwxNIBedv4Xg58A7wPeBOxFKczfDrw8pXRj3hq8IZQkSZI6QbNn5EkpPQScOcWYtZTCfOW2q9iN\n9ptqzshLkiSpE3Td9LQ98pIkSeoEBnlJkiSpgAzykiRJUgF1XZC3R16SJEmdoOuCvDPykiRJ6gRd\nF+T7eg3ykiRJKr6uC/I9YZCXJElS8XVdkO/zhlCSJEnqAF2Xau2RlyRJUifouiBvj7wkSZI6QdcF\neWfkJUmS1Am6L8j7YVdJkiR1gO4L8s7IS5IkqQN0XZC3R16SJEmdoPuCvDPykiRJ6gBdF+R7XUde\nkiRJHaDrUq0fdpUkSVIn6L4gb4+8JEmSOkDXBXl75CVJktQJui7Iu/ykJEmSOkHXBXln5CVJktQJ\nui7I9xjkJUmS1AG6Lsg7Iy9JkqRO0HVB3h55SZIkdYKuC/J93hBKkiRJHaDrUq0T8pIkSeoEXRfk\nwzu7SpIkqQN0VZA3wkuSJKlTdFeQdzZekiRJHaKrgrwkSZLUKboqyDshL0mSpE7RXUG+1QVIkiRJ\nddJVQV6SJEnqFF0V5P2wqyRJkjpFdwX5VhcgSZIk1Ul3BXmTvCRJkjpEVwV5SZIkqVN0VZAPm2sk\nSZLUIboqyJvjJUmS1Cm6Ksib4yVJktQpuivIm+QlSZLUIboryLe6AEmSJKlOuirIG+UlSZLUKboq\nyNtaI0mSpE7RXUG+1QVIkiRJddJVQd4kL0mSpE7RVUHeG0JJkiSpU3RXkDfHS5IkqUN0VZCXJEmS\nOkVXBXkn5CVJktQpuivI21sjSZKkDtFVQV6SJEnqFF0V5J2QlyRJUqforiDf6gIkSZKkOumqIC9J\nkiR1iq4K8n7YVZIkSZ2iu4J8qwuQJEmS6qSrgrxJXpIkSZ2iq4K8OV6SJEmdosuCvFFekiRJnaGr\ngrw5XpIkSZ2iq4K8OV6SJEmdoruCvElekiRJHaKrgrwkSZLUKboqyHtDKEmSJHWK7gryrS5AkiRJ\nqpOuCvKSJElSp+iqIG9njSRJkjpFdwX5VhcgSZIk1UlXBXmn5CVJktQpuirIG+MlSZLUKboqyPf2\nGOUlSZLUGZoe5CNiSURcHhEbImJjRFwREUtzHHdCRPx9RNwXEUMR8VBEXBYRB+d97Xmz+p5f8ZIk\nSVKbaGqQj4g5wHXA4cBZwJuBQ4EfR8TcKQ5/I3Ak8CXgVOAvgeOAWyNiScOKliRJktpQs6eozwZW\nAIellNYARMQdwH3AucDnJjn2MymlD1RuiIgbgYHsvOc1pGJJkiSpDTW7teZ04KZyiAdIKQ0ANwJn\nTHZgSumpGtseBNYBB9a5TkmSJKmtNTvIHwncWWP7amDldE8WEUcAi4G7n2ddkiRJUqE0O8jvA6yv\nsX0QWDCdE0VEH/BVSjPyfz/JuHMi4taIuHXdunXTeQlJkiSpbRV5+ckvA78C/EFKqdZfDgBIKV2S\nUjo+pXT8okWLmledJEmS1EDN/rDremrPvE80U19TRHwaOAc4K6V0TZ1qkyRJkgqj2UF+NaU++Wor\ngbvynCAiPgx8EPijlNKldaxNkiRJKoxmt9ZcBZwYESvKGyJiOXBStm9SEfHHwIXAh1NKX25QjZIk\nSVLba3aQ/xqwFrgyIs6IiNOBK4GHgYvLgyJiWUSMRsR5FdveBHwBuBq4LiJOrHhMe8UbSZIkqcia\n2lqTUtoSEacAnwcuBQK4FnhfSmlzxdAAetn5Lxqvzba/NntUugF4VYPKliRJktpOs3vkSSk9BJw5\nxZi1lEJ75ba3AG9pVF2SJElSkRR5+UlJkiSpaxnkJUmSpAIyyEuSJEkFZJCXJEmSCsggL0mSJBWQ\nQV6SJEkqoEgptbqGpomIdcCDra5DbWFf4OlWF6G24fWgSl4PquT1oLLpXAvLUkqLGlkMdFmQl8oi\n4taU0vGtrkPtwetBlbweVMnrQWXteC3YWiNJkiQVkEFekiRJKiCDvLrVJa0uQG3F60GVvB5UyetB\nZW13LdgjL0mSJBWQM/KSJElSARnk1ZYi4g0R8a8R8XBEbI2IeyPiUxExv2rcgoj4u4h4OiK2RMSP\nIuLoGuebHRGfjYjHs/P9T0S8osa4noj4UESsjYhtEXF7RJw5QY1nR8Q9EbE9q++d9XsHNJmIuDoi\nUkRcWLXd66FLRMSpEfEfEbE5IjZGxK0RcUrFfq+FLhERJ0XENRHxVERsioifRMTbqsZ4PXSgiDgo\nIv4m+z0NZX8uLK8xru1//xHxGxHx0+x8D0bERyKid8o3IaXkw0fbPYCbgH8B/gB4FfA+4Nlse082\nJoD/Ah4Bfhd4LXADpTVeD6o632XZ8WcDvwpcAWwFjq0a9wlgO/AB4GTgYmAcOLVq3NnZ9k9k4y7M\nnr+r1e9dpz+y3/XjQAIurNju9dAlD+BcYAT4PPBq4DXAB4HTvBa66wEck/2+fgyckV0PF2f/f3iX\n10NnPyjlgyeBfwP+Pfu9L68a0/a//+z/YWOUevBPBv4M2AZcNOV70Opfgg8ftR7Aohrb/jD7j/SU\n7PkZ2fOTK8bsBQwCX6rY9uJs3FsrtvUB9wJXVWxbnP2H+bGq170WuKPq2KeAb1aN+3r2P4YZrX7/\nOvUBLACeyP5nXB3kvR664AEsz/5gfd8kY7wWuuQBfBIYBuZVbf8f4H+8Hjr7QTaxl33/DmoH+bb/\n/QM/BW6oGndedm3vP9l7YGuN2lJKaV2NzbdkXw/Mvp4OPJZS+nHFcRuA71L6D5eKcSPAP1WMGwW+\nDbwmImZlm18DzAS+VfW63wKOjoiDs+e/DCyqMe5SYCHwsql+Pu22i4A7U0r/p8Y+r4fu8DZKM1pf\nnWSM10L3mEkp7AxVbd/Ac+3DXg8dKqU0nmNYW//+I2IJcOwE42YAr5vshzPIq0hemX29O/t6JHBn\njXGrgaURMa9i3EBKqfp/9Ksp/cd4SMW47cCaGuMAVlaMo8ZrV49THUXEyyj9q8x7Jhji9dAdXgbc\nA7wpIu6PiNGIWBMRldeF10L3+Aal1okvRcQLImLviCi3RXw+G+P10N3a/fdfc1xKaYDSX1AnvU4M\n8iqEiDgQuAD4UUrp1mzzPsD6GsMHs68Lco7bp+Lrsyn7N60pxlHjnNXjVCcRMZNSD+JfpZTunWCY\n10N3eAFwKPBZ4NPArwM/BL4cEX+SjfFa6BIppTsp9Un/BvAopff+b4F3ppS+nQ3zeuhu7f77n2hc\neduk10nfZDuldpD9bflKYBR4a4vLUWv8BbAHpQ8Mqbv1APOBt6SUrsi2XZetVPEh4IstqkstEBGH\nUloYYTXwTkqfnzgD+GpEbEspXdbK+qRGc0ZebS0i9qDUx7YCeE1K6ZGK3et57m/Slar/djvVuMGK\ncXtHROQYR41zVo9THUTEUuDDwP8GZmX/dL53trv8vBevh27xTPb1h1XbrwH2i4gD8FroJp+k1Nf8\n+pTS91JK16aU/hj4v8AXI6IHr4du1+6//4nGlbdNep0Y5NW2ImIGcDlwPKUlnX5eNWQ1z/WWVVoJ\nPJRS2lwx7uCImFNj3DDP9bmtBmYBL6wxDuCuinHUeO3qcaqPFcBsSh8EWl/xgNLSX+uBo/F66Bar\npx7itdBFjqa0Ushw1fZ+Sh8oXIzXQ7dr999/zXHZvzLOYYrrxCCvtpTNolwGnAL8RkrpphrDrgIO\njIhXVhy3J/D6bF/Zdyl98vu3K8b1AW8Erkkpbc82X01pZuf3q17nDyitlDKQPf8fSktH1Ro3CNyY\n88dUPj+jtK5u9QNK4f5kSv+D9XroDt/Jvr6mavtrgUdSSo/jtdBNngCOyT5HU+mllNbhHsTrodu1\n9e8/pfQQcPsE40aAH0z60zVrrU8fPqbzAL5Ctk44cGLV46BsTA/w38DDwJso/cF+ffYfyJKq832b\n0sztOyitZnA5pf/JH1c17tPZ9j+j9AGqr1Ba6u60qnHvzLZfmI27IHv+nla/d93yYNd15L0euuBB\naYWS6yi12LyT0oddv5ZdD2/xWuiuB/CG7Hf/75R6438d+HK27XNeD53/yK6BN/BcbnhX9vyVRfn9\nA6dm2y/Oxv1pdv7PTvnzt/oX4MNHrQewNvsPstbj/Ipx+1C6ucIgpWWargVeXON8ewCfozR7sw24\nGXhVjXG9wEeAByktL3UH8IYJajwX+EU27j7g3a1+37rpQVWQ93rongewJ6WVSZ6k9E/edwC/57XQ\nnQ9K62xfD6wDNlH6V7x3A71eD53/YOKscH2Rfv/Ab1Gamd8OPETphlC9U/38kR0sSZIkqUDskZck\nSZIKyCAvSZIkFZBBXpIkSSogg7wkSZJUQAZ5SZIkqYAM8pIkSVIBGeQlqctExOsj4ucRsS0iUkTs\nXefz7x0R50fEcfU8ryRpZwZ5Seoi2S3HLwMepXQXzF+mdBOdetob+ChgkJekBuprdQGSpKY6EP5f\nO/cWYlUVx3H8+8toummIVEialoo9WD0FGWm99dDNpEAGxDFFq4foJQhKrOhCFJhRdHG0zEulRkV2\nMS0dCRmz0ITCHkztJmXiOIyRCf17WGvb4XAmzjmeyXPk94HDPnvttdf6s1/mv9f8z2IwsCoiNp/s\nYGohqS0ijp7sOMzMmoVX5M3MGiiXlISkyyStk3RE0g+SZubr0yXtktQnaaOkMWX3T5P0maQDuc92\nSTPK+szKc0wpaRskqUvSbklD+osN2JtPF+cxNpVcnyqpW9IfknokrZZ0cS3xSRoN7Mmni/IcIakj\nX98r6bUKsUWOr/w5TsjPsQ9YVWOs7Tm+Pkm9uZxobqVnY2bWipzIm5kNjNXAB8AU4CtgiaQngLuB\nB4CZwHhgZdl9Y4B3gen53veBTkl3FR0iYnEev1PSRbl5HnAN0B4Rvf3E1Anckb8/RiqruQcgj/82\n8C1wOzAXmAB0SRpcQ3z7gan5+5N5jon5WdTjPaALuAVYUG2skq4Flud7p+R+i0hlP2ZmpwSX1piZ\nDYynI+J1AElfAjeTEs5LikRb0nBgoaRREbEPICIeLwaQdBqwCRhOegF4qWT8OcDXwDJJjwAPAfMi\nYmt/AUXET5J25NPdEdGd5zkXeAp4NSLuLJn/C+A7YBbwbDXxRcRRSdtzl++LOU7AcxGxsGTOamO9\nGuiJiPtKxvrkBGMxM2sqXpE3MxsYHxVfIuIQ8BvQXbZavisfRxYNksZJekPSz8Cx/JlNWr0/LiJ6\ngHZgMrAO2ExKcOsxERgCrJB0evEBfswxTq41vgZ6p85YtwFDJS2XdFOjd+YxM2sGTuTNzAbGobLz\nv/ppAzgTjq82rweuJJXfTAKuApYAbRXm6CatQreRVq7/rjPWC/JxA/8m58XncmBYnfE1wv56Yo2I\nLlIZ0UjSy8ABSRskXTFAcZqZ/e9cWmNm1jwmAqOASRHxedGYV5wrmQ+MA3YCCyRtjIjDdcx7MB87\ngG8qXC+2p6w1vkr+BM4obZA07D/6R52xEhFrgDX5BeR60n8sPpY04gReeszMmoYTeTOz5nF2Ph4r\nGiQNBW4t7yhpEvAgaWX8LVK9/IukcptabSElwGMjYmkD4iu2iDyrwhj7SD9MLXVj9aFWHetxEdEH\nrJV0KbCQtGp/oIY5zcyakhN5M7PmsQXoBV6QNB84h/Qj1t+B84pOOXleAXwKPBMRIWkOsErSumoT\n3EJE9Eq6P897Pqm+/zBpz/nrgE0RsbLa+IBfSSvn0yTtBI4AeyLiIPAmaQefBcBaUplOR6NjlfQo\ncCGwEfgFGAHcC+yICCfxZnZKcI28mVmTyAnmbcAgYA1p+8ZO0jaKpV4hrXbPiIjI964GFgPPSxpb\nx9wvk7Z4HA8sAz4EHiYt+OyoJb5ctjIbGEqqZd9G2rUHYCmpJGgqaevKG/KYDY0V2AqMJm1ZuZ5U\nVtNFbav/ZmZNTflvgJmZmZmZtRCvyJuZmZmZtSAn8mZmZmZmLciJvJmZmZlZC3Iib2ZmZmbWgpzI\nm5mZmZm1ICfyZmZmZmYtyIm8mZmZmVkLciJvZmZmZtaCnMibmZmZmbWgfwBUfstUqxs1XQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a14c72a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(results).plot(figsize=(12,8), fontsize=16)\n",
    "plt.xlabel('max features', fontsize=16)\n",
    "plt.ylabel('out of sample accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Named entity recognition (40 %)\n",
    "\n",
    "Named entity recognition is a common NLP task that tries to identify entities in text.\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Named-entity_recognition\n",
    "\n",
    "Common Types of entities include `Locations`, `People`, and `Organizations`. For example, in the sentence\n",
    "# Janet Yellen, the chairwoman of the Federal Reserve, gave a speech in Colorado.\n",
    "## $ \\\\ $ \n",
    "the goal would be to recognize\n",
    "# `Janet Yellen`$_{PERSON}$, the chairwoman of the `Federal Reserve`$_{ORGANIZATION}$, gave a speech in `Colorado`$_{LOCATION}$.\n",
    "# $ \\\\ $\n",
    "# In this problem we will build a model to recognized named entities using word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "### Give an example of a sentence with a Person but not a location. \n",
    "### Give an example of a sentence with an organization and a location, but not a person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# put it here!\n",
    "The US President walked away.\n",
    "\n",
    "The Federal Reserve is located in Washington DC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: building a model\n",
    "# $ \\\\ $\n",
    "### The goal of this section is to build a model to take a sentence (list of words) and identify what kind of tag each word is\n",
    "# $ \\\\ $\n",
    "## Why is this problem hard:\n",
    "### Some words will be the same tag all the time. For example `Colorado` is almost always a `LOCATION`\n",
    "### Some words depend on context: above `federal` and `reserve` are `ORGANIZATION` but I can write `I would like to reserve a table.`\n",
    "\n",
    "# $ \\\\ $\n",
    "## To combat this issue we will make a very simple model but taking a 3-word window around every word\n",
    "  - For every word, we will take the word vector of that word and the two surrounding words\n",
    "\n",
    "### Example: `I went to the store` will be represented as \n",
    " - `I` $\\rightarrow$ `UKNOWN` - `I` - `went` $\\rightarrow$ $\\left[ V_{UKNOWN}, V_{I}, V_{went}\\right]$\n",
    " - `went` $\\rightarrow$ `I` - `went` - `to`$\\rightarrow$ $\\left[V_{I}, V_{went},  V_{to}\\right]$\n",
    " - `to` $\\rightarrow$ `went` - `to` - `the`$\\rightarrow$ $\\left[V_{went},  V_{to}, V_{the}\\right]$\n",
    " - ...\n",
    "#### Where \n",
    " - $V_{word_{i}}$ is the representation for $word_{i}$\n",
    " - `UKNOWN` is the token for unknown or boundary words\n",
    " \n",
    "Like this, we will encode some __context__ around every word. Each word here will be encoded as a $3 * d_{embedding}$-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    \"\"\"Load a glove embedding from a file\"\"\"\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "def load_dataset(fname):\n",
    "    \"\"\"Load an NER dataset\"\"\"\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            line = line.lower()\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\".lower(), line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        if cur:\n",
    "            docs.append(cur)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLOVE_DIR = '/home/dan/MFin/MLfin/'  # FIXME directory with glove\n",
    "DATA_PATH = '/home/dan/MFin/MLfin/ml_finance/hw2/train.conll'  # where you downloaded the data\n",
    "\n",
    "word_vecs = load_glove_file(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "docs = load_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_first_doc = [\n",
    "    ['eu', 'org'],\n",
    "    ['rejects', 'o'],\n",
    "    ['german', 'misc'],\n",
    "    ['call', 'o'],\n",
    "    ['to', 'o'],\n",
    "    ['boycott', 'o'],\n",
    "    ['british', 'misc'],\n",
    "    ['lamb', 'o'],\n",
    "    ['.', 'o'],\n",
    "]\n",
    "assert len(word_vecs) == 400000, 'word vectors did not load properly'\n",
    "assert word_vecs['the'].shape == (50,), 'word vectors did not load properly'\n",
    "assert len(docs) == 14041, 'something has gone wrong with data loading'\n",
    "assert docs[0] == correct_first_doc, 'something has gone wrong with data loading'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = len(word_vecs)  # max number of words to use in the embedding\n",
    "UNKNOWN = 'UUUNKKK'.lower()  # token for unknown word\n",
    "UNKNOWN_WORD_INDEX = 0\n",
    "EMBEDDING_DIM = 50  # dimension of embedding\n",
    "NULL_TAG = 'o'  # tags that are not a named entity\n",
    "\n",
    "# Some derived quantities\n",
    "TAGS = (NULL_TAG, 'loc', 'per', 'org', 'misc')\n",
    "NUM_TO_TAG = dict(enumerate(TAGS))\n",
    "TAG_TO_NUM = {tag: num for num, tag in NUM_TO_TAG.items()}\n",
    "\n",
    "NUM_CLASSES = len(TAGS)\n",
    "assert NUM_CLASSES == 5, 'somethig has gone wrong'\n",
    "\n",
    "WINDOW = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {word: idx + 1 for idx, word in enumerate(word_vecs.keys())}\n",
    "num_to_word = {num: word for word, num in word_to_num.items()}\n",
    "\n",
    "word_to_num[UNKNOWN] = UNKNOWN_WORD_INDEX\n",
    "num_to_word[UNKNOWN_WORD_INDEX] = UNKNOWN\n",
    "\n",
    "assert word_to_num['the'] > 10, '\"the\" is not a common word- something has gone wrong.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shihad'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_to_num.items():#tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating windowed-word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_windows(words, tags, word_to_num, tag_to_num, left=WINDOW, right=WINDOW):\n",
    "    \"\"\"Turn sequences of words and tags into corresponding windowed sequences\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    word_dict = {ind: word for ind, word in enumerate(words)}\n",
    "    for i, word in enumerate(words):\n",
    "        if word == \"<s>\" or word == \"</s>\":\n",
    "            continue # skip sentence delimiters\n",
    "        word_seq = [word_dict.get(i + ii, UNKNOWN) for ii in range(-left, 1 + right)]\n",
    "        int_seq = [word_to_num.get(w, UNKNOWN_WORD_INDEX) for w in word_seq]\n",
    "        tagn = tag_to_num[tags[i]] \n",
    "        X.append(int_seq)\n",
    "        y.append(tagn)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "def window_row_to_vector(window_row, embed_matrix):\n",
    "    \"\"\"Turn a row of integers (np.array) into a single word vector\"\"\"\n",
    "    # TODO: implement this\n",
    "    return np.hstack([embed_matrix[i] for i in window_row]) # to be removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = zip(*docs[0])\n",
    "x, y = seq_to_windows(words, tags, word_to_num=word_to_num, tag_to_num=TAG_TO_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.dtype == np.int, 'x has the wrong data type'\n",
    "reconstructed_words = [num_to_word[num] for num in x[:, WINDOW]]\n",
    "assert tuple(reconstructed_words) == words, 'word transformation has gone wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xs = []\n",
    "all_ys = []\n",
    "for doc in docs:\n",
    "#     TODO\n",
    "#     1. unpack the words and the tags from `docs`\n",
    "#     2. use `seq_to_windows` to turn `words` and `tag` into `x` and `y`\n",
    "#     3. turn `x` into a single vector with `window_row_to_vector`\n",
    "    \n",
    "    words, tags = zip(*doc)\n",
    "    \n",
    "    # Your code here\n",
    "    x,y = seq_to_windows(words,tags,word_to_num,TAG_TO_NUM)\n",
    "    \n",
    "    x = window_row_to_vector(x.T,embedding_matrix)\n",
    "    \n",
    "    all_xs.extend(x)\n",
    "    all_ys.extend(y)\n",
    "    \n",
    "    \n",
    "all_xs = np.vstack(all_xs)\n",
    "all_ys = np.vstack(all_ys)\n",
    "\n",
    "\n",
    "all_ys = to_categorical(all_ys)\n",
    "assert all_xs.shape[0] == all_ys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. make an array of indices to be shuffled with `np.arange`\n",
    "# 2. shuffle the indices randomly\n",
    "# 3. use the shuffled indices to shuffle `all_xs` and `all_ys`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "inds = np.arange(all_xs.shape[0])\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "all_xs, all_ys = all_xs[inds], all_ys[inds]\n",
    "\n",
    "\n",
    "cut = int(0.8 * all_xs.shape[0])\n",
    "x_train, x_val = all_xs[:cut], all_xs[cut:]\n",
    "y_train, y_val = all_ys[:cut], all_ys[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#TODO\n",
    "# 1. build a network with \n",
    "#  - an input layer\n",
    "#  - some number of dense layers and dropout\n",
    "\n",
    "word_input = Input(shape=(x_train.shape[1],))  # to be removed\n",
    "\n",
    "x = Dense(100, activation='relu')(word_input)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162896 samples, validate on 40725 samples\n",
      "Epoch 1/16\n",
      "162896/162896 [==============================] - 3s 20us/step - loss: 0.2880 - acc: 0.9101 - val_loss: 0.1879 - val_acc: 0.9432\n",
      "Epoch 2/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1866 - acc: 0.9423 - val_loss: 0.1605 - val_acc: 0.9502\n",
      "Epoch 3/16\n",
      "162896/162896 [==============================] - 2s 12us/step - loss: 0.1654 - acc: 0.9489 - val_loss: 0.1476 - val_acc: 0.9547\n",
      "Epoch 4/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1529 - acc: 0.9530 - val_loss: 0.1399 - val_acc: 0.9581\n",
      "Epoch 5/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1467 - acc: 0.9552 - val_loss: 0.1352 - val_acc: 0.9599\n",
      "Epoch 6/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1399 - acc: 0.9569 - val_loss: 0.1308 - val_acc: 0.9606\n",
      "Epoch 7/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1355 - acc: 0.9586 - val_loss: 0.1282 - val_acc: 0.9617\n",
      "Epoch 8/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1322 - acc: 0.9600 - val_loss: 0.1264 - val_acc: 0.9626\n",
      "Epoch 9/16\n",
      "162896/162896 [==============================] - 2s 12us/step - loss: 0.1290 - acc: 0.9605 - val_loss: 0.1252 - val_acc: 0.9628\n",
      "Epoch 10/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1267 - acc: 0.9613 - val_loss: 0.1260 - val_acc: 0.9625\n",
      "Epoch 11/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1255 - acc: 0.9614 - val_loss: 0.1269 - val_acc: 0.9629\n",
      "Epoch 12/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1231 - acc: 0.9620 - val_loss: 0.1219 - val_acc: 0.9640\n",
      "Epoch 13/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1206 - acc: 0.9629 - val_loss: 0.1230 - val_acc: 0.9641\n",
      "Epoch 14/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1197 - acc: 0.9632 - val_loss: 0.1208 - val_acc: 0.9650\n",
      "Epoch 15/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1176 - acc: 0.9638 - val_loss: 0.1225 - val_acc: 0.9648\n",
      "Epoch 16/16\n",
      "162896/162896 [==============================] - 2s 11us/step - loss: 0.1175 - acc: 0.9636 - val_loss: 0.1240 - val_acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79fd0070f0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=True, epochs=16, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('swansea', '1', 'lincoln', '2')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to process new sentences so that they can be processed by our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Preprocess a sentence into word vectors for the model\n",
    "    \n",
    "    TODO:\n",
    "        1. split sentence into words (and make lowercase)\n",
    "        2. make each word into a 3-word window (use seq_to_windows)\n",
    "            - this will require the creating some fake tags in the right format\n",
    "                in order to pass to `seq_to_windows`\n",
    "        3. turn each row (3-word window) into a single vector (use window_row_to_vector)\n",
    "        4. turn the list of 150-d vectors into a numpy matrix shape (n_words x 150)\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    # your code here\n",
    "    fake_tags =tuple(random.choice(list(TAG_TO_NUM.keys()),len(words)))\n",
    "    # your code here\n",
    "    x, y = seq_to_windows(words, fake_tags,word_to_num,TAG_TO_NUM)\n",
    "    \n",
    "    x = window_row_to_vector(window_row=x.T, embed_matrix=embedding_matrix)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    preprocess_sentence('This sentence has five words').shape == \n",
    "    (5, EMBEDDING_DIM * (1 + 2 * WINDOW))\n",
    "), '`preprocess_sentence` does not work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan       :  per\n",
      "Smith     :  per\n",
      "quit      :  o\n",
      "after     :  o\n",
      "Sears     :  org\n",
      "Inc       :  org\n",
      "announced :  o\n",
      "it        :  o\n",
      "would     :  o\n",
      "acquire   :  o\n",
      "The       :  o\n",
      "Federal   :  o\n",
      "Reserve   :  org\n",
      "Inc       :  org\n",
      "for       :  o\n",
      "17        :  o\n",
      "dollars   :  o\n",
      ".         :  o\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Come up with a sentence\n",
    "# 2. preprocess it for consumption by the network with `preprocess_sentence`\n",
    "# 3. use the model to make predictions\n",
    "# 4. Turn the predicted probabilities into predicted labels\n",
    "# 5. print the output nicely  (done for you)\n",
    "\n",
    "\n",
    "# Make up some of your own sentences\n",
    "sentence = 'Dan Smith quit after Sears Inc announced it would acquire The Federal Reserve Inc for 17 dollars .'\n",
    "\n",
    "processed_sentence = preprocess_sentence(sentence)\n",
    "predictions = model.predict(processed_sentence)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "maxlen = max(map(len, sentence.split()))\n",
    "for word, label in zip(sentence.split(), predicted_labels):\n",
    "    print('{} :  {}'.format(word.ljust(maxlen), NUM_TO_TAG[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-56bfd15e456c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNUM_TO_TAG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "NUM_TO_TAG[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

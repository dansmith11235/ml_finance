{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1- small data learning with embeddings (30%)\n",
    "### We're going to use pre-trained embeddings to try to learn a text classification problem with few training examples\n",
    "### This is very similar to what we did in class!\n",
    "## $ \\\\ $\n",
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=1234)\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "### a. What is the most common class in the train set?\n",
    "### b. What is the out of sample (test) accuracy if we guess the most probable class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common class 10: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "# hint: you can do this in many ways, including collections.Counter or pandas\n",
    "count_dict = Counter(data_train['target'])\n",
    "most_common_class = max(count_dict, key=count_dict.get)\n",
    "print('most common class {}: {}'.format(most_common_class, data_train.target_names[most_common_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05297397769516728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the accuracy score\n",
    "# acc would just be the % of class 10 in test data as we would pred this all the time\n",
    "test_count_dict = Counter(data_test['target'])\n",
    "test_count_dict[most_common_class]/ len(data_test['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Turn the text into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Instantiate a tokenizer with max workds\n",
    "# 2. fit the tokenizer on text\n",
    "# 3. Turn the text into integer sequences (train and test)\n",
    "# 4. pad the sequences to a constant sequence length (train and test)\n",
    "# 5. turn y into categorical variables\n",
    "\n",
    "\n",
    "# you should have 4 variables:\n",
    "# y_train, y_test, int_sequences_train, int_sequences_test\n",
    "# all are numpy arrays\n",
    "\n",
    "tok = Tokenizer(num_words=MAX_WORDS)\n",
    "tok.fit_on_texts(data_train['data'])  # Fitting tokenizer on train\n",
    "seq_train, seq_test = tok.texts_to_sequences(data_train['data']), tok.texts_to_sequences(data_test['data'])\n",
    "int_sequences_train = pad_sequences(seq_train, maxlen=MAX_SEQ_LEN)\n",
    "int_sequences_test = pad_sequences(seq_test, maxlen=MAX_SEQ_LEN)\n",
    "y_train, y_test = to_categorical(data_train['target']), to_categorical(data_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.shape ==(11314, 20), 'something went wrong'\n",
    "assert int_sequences_test.shape == (7532, 100), 'something went wrong'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: load the GloVe embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLOVE_DIR = '/Users/Hugo/Documents/data/text/glove.6B/'  # FIXME directory with glove\n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "word_vecs = load_glove_file(GLOVE_PATH)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = y_train.shape[1]\n",
    "assert NUM_CLASSES == 20, 'something went wrong'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 100)          5100      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 507,120\n",
      "Trainable params: 7,120\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dropout, Dense, GlobalAveragePooling1D\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Build a model with\n",
    "#  - an embedding\n",
    "#  - some number of dense layers\n",
    "#  - dropout\n",
    "#  - don't forget to use GlobalAveragePooling to average over one dimension\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "x = Embedding(input_dim=MAX_WORDS, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, \n",
    "              trainable=False)(word_input)\n",
    "x = Dense(100, activation=\"tanh\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "output = Dense(NUM_CLASSES, activation=\"softmax\")(avg_pool)\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507120"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_train = 100\n",
    "epochs = 1000  # this is a big number but won't take long with 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2261019649495486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train],\n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you should be able to get more than 20% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare to others methods\n",
    "### a. How does this compare to a randomly initialized, trainable embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 100)          5100      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 507,120\n",
      "Trainable params: 507,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Build the same model as above, but with a random embedding\n",
    "\n",
    "random_emb_matrix = np.random.random((MAX_WORDS, EMBEDDING_DIM))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "# your code here\n",
    "\n",
    "x = Embedding(input_dim=MAX_WORDS, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              weights=[random_emb_matrix],\n",
    "              input_length=MAX_SEQ_LEN, \n",
    "              trainable=True)(word_input)\n",
    "x = Dense(100, activation=\"tanh\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "output = Dense(NUM_CLASSES, activation=\"softmax\")(avg_pool)\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1249336165693043"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b: how does this compare to logistic regression trained on 100 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12413701540095592"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
    "sample_train_data = data_train['data'][:100]\n",
    "sample_train_target = data_train['target'][:100]\n",
    "\n",
    "# TODO\n",
    "# 1. make a count vectorizer\n",
    "# 2. fit it on only `samples_to_train` data points\n",
    "# 3. trainsform train and test data into integers\n",
    "# 4. fit logistic regression on just `num_samples_to_train` samples\n",
    "# 5. Compute accuracy score\n",
    "\n",
    "vec = CountVectorizer().fit(sample_train_data)\n",
    "x_train, x_test = vec.transform(sample_train_data), vec.transform(data_test['data'])\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, sample_train_target)\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This should be approximately 10-12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Homework problem: improving BOW (30%)\n",
    "There are many improvements that can be made to the bag of words represetation, without resorting to neural networks.\n",
    "Here we'll try one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe to restart notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: fit a bag of words and logistic regression to the 20 newsgroups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "# 1. make a count vectorizer with max_features=20000\n",
    "# 2. fit it\n",
    "# 3. transform the train and test data into number\n",
    "vec = CountVectorizer(max_features=20000)\n",
    "vec.fit(data_train.data)\n",
    "xtr = vec.transform(data_train.data)\n",
    "xte = vec.transform(data_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.606744556558683"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. fit logistic regression\n",
    "# 2. compute accuracy score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtr, data_train.target)   # to be removed\n",
    "accuracy_score(data_test.target, lr.predict(xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TFIDF\n",
    "A big problem with counting words is that we'll tend to overweight very common words. \n",
    "These common words often carry little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 93969),\n",
       " ('to', 51191),\n",
       " ('of', 45608),\n",
       " ('a', 40042),\n",
       " ('and', 39197),\n",
       " ('is', 28204),\n",
       " ('in', 27756),\n",
       " ('I', 27143),\n",
       " ('that', 25016),\n",
       " ('for', 18066)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def word_iterator():\n",
    "    \"\"\"This iterator yields one word at a time from the train data\"\"\"\n",
    "    for doc in data_train.data:\n",
    "        for word in doc.split():\n",
    "            yield word\n",
    "\n",
    "Counter(word_iterator()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF is a scheme that combats this. \n",
    "## TFIDF = $\\text{Term Frequency Inverse Document Frequency} $\n",
    "# $ \\\\ $\n",
    "\n",
    "# $TFIDF\\left(d, t\\right) \\equiv \\frac{\\text{ Count}\\left(d, t\\right)}{\\text{Doc-Freq}\\left(d, t\\right)} \\equiv \\text{ Count}\\left(d, t\\right)\\,\\left(1 + log\\left( \\frac{N_{docs}}{df_{t}}\\right)\\right)$\n",
    "## Where\n",
    "### $df_{t}$ is the number of documents in which term $t$ appears\n",
    "### $N_{docs}$ is the total number of documents\n",
    "### $\\text{Count}\\left(d,t\\right)$ is the number of times term $t$ appears in document $d$ (the count matrix)\n",
    "\n",
    "# $ \\\\ $ \n",
    "# $ \\\\ $ \n",
    "## Like this, we suppress the weight of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: write turn the count matrix into a TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_vector(count_matrix):\n",
    "    \"\"\"Get the inverse document frequence vector (shape = num_words)\"\"\"\n",
    "    df = np.array((count_matrix > 0).astype(int).sum(axis=0))\n",
    "    return np.log(count_matrix.shape[0] / (1+df))\n",
    "\n",
    "\n",
    "#TODO(fill in this function)\n",
    "def get_tfidf_matrix(count_matrix):\n",
    "    \"\"\"Turn a count matrix into a tfidf matrix\"\"\"\n",
    "    # TODO\n",
    "#     1. get the idfs with the aboce function\n",
    "#     2. turn it into a numpy array `with .toarray()`\n",
    "#     3. loop the the ROWS of the matrix and transform them\n",
    "        # YOUR CODE HERE\n",
    "    idf = get_idf_vector(count_matrix=count_matrix)\n",
    "    count_matrix = count_matrix.toarray()\n",
    "    for i,r in enumerate(count_matrix):\n",
    "        count_matrix[i] = r * idf\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_transformed = get_tfidf_matrix(xtr)\n",
    "xte_transformed = get_tfidf_matrix(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6263940520446096"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(xtr_transformed, data_train.target)\n",
    "accuracy_score(data_test.target, lr.predict(xte_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Do the same with scikitlearn's implmenetation\n",
    "### Happily, sklearn does this for us\n",
    "### And it includes some other nice normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 989884 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6715347849176846"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TODO:\n",
    "# 1. instantiate a TfidfVectorizer with max_features = 20000\n",
    "# 2. fit it on the train data\n",
    "# 3. transform train and test data into matrices\n",
    "# 4. fit logistic regression on the train data\n",
    "# 5. compute the accuracy score on the test data\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(max_features=20000)\n",
    "vec.fit(data_train.data)\n",
    "x_train, x_test = vec.transform(data_train.data), vec.transform(data_test.data)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, data_train.target)\n",
    "# Your code here\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tuning the number of words to use\n",
    "## Make a plot of how the vocabulary size (`max_features`) impacts results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 57.05 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "for max_features in (100, 500, 1000, 5000, 10000, 20000, 50000, None):\n",
    "    # TODO:\n",
    "    # 1. instantiate a TfidfVectorizer with max_features = max_features\n",
    "    # 2. fit it on the train data\n",
    "    # 3. transform train and test data into matrices\n",
    "    # 4. fit logistic regression on the train data\n",
    "    # 5. compute the accuracy score on the test data\n",
    "    \n",
    "    # your code here\n",
    "    vec = TfidfVectorizer(max_features=max_features)\n",
    "    vec.fit(data_train.data)\n",
    "    x_train, x_test = vec.transform(data_train.data), vec.transform(data_test.data)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, data_train.target)\n",
    "    \n",
    "    if max_features is None:\n",
    "        num_features = len(vec.get_feature_names())\n",
    "    else:\n",
    "        num_features = max_features\n",
    "    results[num_features] = accuracy_score(data_test.target, lr.predict(x_test)) # to be removed\n",
    "\n",
    "print('this took {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'out of sample accuracy')"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(results).plot(figsize=(12,8), fontsize=16)\n",
    "plt.xlabel('max features', fontsize=16)\n",
    "plt.ylabel('out of sample accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Named entity recognition (40 %)\n",
    "\n",
    "Named entity recognition is a common NLP task that tries to identify entities in text.\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Named-entity_recognition\n",
    "\n",
    "Common Types of entities include `Locations`, `People`, and `Organizations`. For example, in the sentence\n",
    "# Janet Yellen, the chairwoman of the Federal Reserve, gave a speech in Colorado.\n",
    "## $ \\\\ $ \n",
    "the goal would be to recognize\n",
    "# `Janet Yellen`$_{PERSON}$, the chairwoman of the `Federal Reserve`$_{ORGANIZATION}$, gave a speech in `Colorado`$_{LOCATION}$.\n",
    "# $ \\\\ $\n",
    "# In this problem we will build a model to recognized named entities using word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "### Give an example of a sentence with a Person but not a location. \n",
    "### Give an example of a sentence with an organization and a location, but not a person. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# put it here!\n",
    "ALEX EATS A PIZZA\n",
    "MIT IS A SCHOOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: building a model\n",
    "# $ \\\\ $\n",
    "### The goal of this section is to build a model to take a sentence (list of words) and identify what kind of tag each word is\n",
    "# $ \\\\ $\n",
    "## Why is this problem hard:\n",
    "### Some words will be the same tag all the time. For example `Colorado` is almost always a `LOCATION`\n",
    "### Some words depend on context: above `federal` and `reserve` are `ORGANIZATION` but I can write `I would like to reserve a table.`\n",
    "\n",
    "# $ \\\\ $\n",
    "## To combat this issue we will make a very simple model but taking a 3-word window around every word\n",
    "  - For every word, we will take the word vector of that word and the two surrounding words\n",
    "\n",
    "### Example: `I went to the store` will be represented as \n",
    " - `I` $\\rightarrow$ `UKNOWN` - `I` - `went` $\\rightarrow$ $\\left[ V_{UKNOWN}, V_{I}, V_{went}\\right]$\n",
    " - `went` $\\rightarrow$ `I` - `went` - `to`$\\rightarrow$ $\\left[V_{I}, V_{went},  V_{to}\\right]$\n",
    " - `to` $\\rightarrow$ `went` - `to` - `the`$\\rightarrow$ $\\left[V_{went},  V_{to}, V_{the}\\right]$\n",
    " - ...\n",
    "#### Where \n",
    " - $V_{word_{i}}$ is the representation for $word_{i}$\n",
    " - `UKNOWN` is the token for unknown or boundary words\n",
    " \n",
    "Like this, we will encode some __context__ around every word. Each word here will be encoded as a $3 * d_{embedding}$-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    \"\"\"Load a glove embedding from a file\"\"\"\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "def load_dataset(fname):\n",
    "    \"\"\"Load an NER dataset\"\"\"\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            line = line.lower()\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\".lower(), line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        if cur:\n",
    "            docs.append(cur)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLOVE_DIR = '/Users/Hugo/Documents/data/text/glove.6B/'  # FIXME directory with glove\n",
    "DATA_PATH = '/Users/Hugo/Documents/Python/mit_classes/analysis_ml/ml_finance/hw2/train.conll'  # where you downloaded the data\n",
    "\n",
    "word_vecs = load_glove_file(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "docs = load_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_first_doc = [\n",
    "    ['eu', 'org'],\n",
    "    ['rejects', 'o'],\n",
    "    ['german', 'misc'],\n",
    "    ['call', 'o'],\n",
    "    ['to', 'o'],\n",
    "    ['boycott', 'o'],\n",
    "    ['british', 'misc'],\n",
    "    ['lamb', 'o'],\n",
    "    ['.', 'o'],\n",
    "]\n",
    "assert len(word_vecs) == 400000, 'word vectors did not load properly'\n",
    "assert word_vecs['the'].shape == (50,), 'word vectors did not load properly'\n",
    "assert len(docs) == 14041, 'something has gone wrong with data loading'\n",
    "assert docs[0] == correct_first_doc, 'something has gone wrong with data loading'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = len(word_vecs)  # max number of words to use in the embedding\n",
    "UNKNOWN = 'UUUNKKK'.lower()  # token for unknown word\n",
    "UNKNOWN_WORD_INDEX = 0\n",
    "EMBEDDING_DIM = 50  # dimension of embedding\n",
    "NULL_TAG = 'o'  # tags that are not a named entity\n",
    "\n",
    "# Some derived quantities\n",
    "TAGS = (NULL_TAG, 'loc', 'per', 'org', 'misc')\n",
    "NUM_TO_TAG = dict(enumerate(TAGS))\n",
    "TAG_TO_NUM = {tag: num for num, tag in NUM_TO_TAG.items()}\n",
    "\n",
    "NUM_CLASSES = len(TAGS)\n",
    "assert NUM_CLASSES == 5, 'somethig has gone wrong'\n",
    "\n",
    "WINDOW = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {word: idx + 1 for idx, word in enumerate(word_vecs.keys())}\n",
    "num_to_word = {num: word for word, num in word_to_num.items()}\n",
    "\n",
    "word_to_num[UNKNOWN] = UNKNOWN_WORD_INDEX\n",
    "num_to_word[UNKNOWN_WORD_INDEX] = UNKNOWN\n",
    "\n",
    "assert word_to_num['the'] < 10, '\"the\" is not a common word- something has gone wrong.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_to_num.items():#tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating windowed-word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_windows(words, tags, word_to_num, tag_to_num, left=WINDOW, right=WINDOW):\n",
    "    \"\"\"Turn sequences of words and tags into corresponding windowed sequences\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    word_dict = {ind: word for ind, word in enumerate(words)}\n",
    "    for i, word in enumerate(words):\n",
    "        if word == \"<s>\" or word == \"</s>\":\n",
    "            continue # skip sentence delimiters\n",
    "        word_seq = [word_dict.get(i + ii, UNKNOWN) for ii in range(-left, 1 + right)]\n",
    "        int_seq = [word_to_num.get(w, UNKNOWN_WORD_INDEX) for w in word_seq]\n",
    "        tagn = tag_to_num[tags[i]] \n",
    "        X.append(int_seq)\n",
    "        y.append(tagn)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "def window_row_to_vector(window_row, embed_matrix):\n",
    "    \"\"\"Turn a row of integers (np.array) into a single word vector\"\"\"\n",
    "    # TODO: implement this\n",
    "    return np.hstack([embed_matrix[i] for i in window_row]) # to be removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = zip(*docs[0])\n",
    "x, y = seq_to_windows(words, tags, word_to_num=word_to_num, tag_to_num=TAG_TO_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.dtype == np.int, 'x has the wrong data type'\n",
    "reconstructed_words = [num_to_word[num] for num in x[:, WINDOW]]\n",
    "assert tuple(reconstructed_words) == words, 'word transformation has gone wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xs = []\n",
    "all_ys = []\n",
    "for doc in docs:\n",
    "#     TODO\n",
    "#     1. unpack the words and the tags from `docs`\n",
    "#     2. use `seq_to_windows` to turn `words` and `tag` into `x` and `y`\n",
    "#     3. turn `x` into a single vector with `window_row_to_vector`\n",
    "    \n",
    "    words, tags = zip(*doc)\n",
    "    x, y = seq_to_windows(words=words, tags=tags, word_to_num=word_to_num, tag_to_num=TAG_TO_NUM)\n",
    "    x = window_row_to_vector(window_row=x.T, embed_matrix=embedding_matrix)\n",
    "    # Your code here\n",
    "    \n",
    "    all_xs.extend(x)\n",
    "    all_ys.extend(y)\n",
    "    \n",
    "    \n",
    "all_xs = np.vstack(all_xs)\n",
    "all_ys = np.vstack(all_ys)\n",
    "\n",
    "\n",
    "all_ys = to_categorical(all_ys)\n",
    "assert all_xs.shape[0] == all_ys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. make an array of indices to be shuffled with `np.arange`\n",
    "# 2. shuffle the indices randomly\n",
    "# 3. use the shuffled indices to shuffle `all_xs` and `all_ys`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "inds = np.arange(0, len(all_xs))\n",
    "shuffle(inds)\n",
    "all_xs, all_ys = all_xs[inds], all_ys[inds]\n",
    "\n",
    "cut = int(0.8 * all_xs.shape[0])\n",
    "x_train, x_val = all_xs[:cut], all_xs[cut:]\n",
    "y_train, y_val = all_ys[:cut], all_ys[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#TODO\n",
    "# 1. build a network with \n",
    "#  - an input layer\n",
    "#  - some number of dense layers and dropout\n",
    "\n",
    "word_input = Input(shape=(x_train.shape[1],))  # to be removed\n",
    "x = Dense(64, activation='tanh')(word_input)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162896 samples, validate on 40725 samples\n",
      "Epoch 1/16\n",
      "162896/162896 [==============================] - 2s 10us/step - loss: 0.3086 - acc: 0.9029 - val_loss: 0.2328 - val_acc: 0.9260\n",
      "Epoch 2/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.2234 - acc: 0.9310 - val_loss: 0.2031 - val_acc: 0.9374\n",
      "Epoch 3/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.2023 - acc: 0.9375 - val_loss: 0.1868 - val_acc: 0.9428\n",
      "Epoch 4/16\n",
      "162896/162896 [==============================] - 1s 8us/step - loss: 0.1904 - acc: 0.9415 - val_loss: 0.1763 - val_acc: 0.9465\n",
      "Epoch 5/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1799 - acc: 0.9444 - val_loss: 0.1703 - val_acc: 0.9484\n",
      "Epoch 6/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1740 - acc: 0.9465 - val_loss: 0.1624 - val_acc: 0.9504\n",
      "Epoch 7/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1678 - acc: 0.9487 - val_loss: 0.1594 - val_acc: 0.9509\n",
      "Epoch 8/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1636 - acc: 0.9496 - val_loss: 0.1541 - val_acc: 0.9532\n",
      "Epoch 9/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1598 - acc: 0.9501 - val_loss: 0.1516 - val_acc: 0.9538\n",
      "Epoch 10/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1573 - acc: 0.9514 - val_loss: 0.1482 - val_acc: 0.9546\n",
      "Epoch 11/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1535 - acc: 0.9526 - val_loss: 0.1467 - val_acc: 0.9553\n",
      "Epoch 12/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1527 - acc: 0.9530 - val_loss: 0.1443 - val_acc: 0.9562\n",
      "Epoch 13/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1501 - acc: 0.9536 - val_loss: 0.1428 - val_acc: 0.9561\n",
      "Epoch 14/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1484 - acc: 0.9540 - val_loss: 0.1442 - val_acc: 0.9566\n",
      "Epoch 15/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1450 - acc: 0.9552 - val_loss: 0.1417 - val_acc: 0.9572\n",
      "Epoch 16/16\n",
      "162896/162896 [==============================] - 1s 7us/step - loss: 0.1446 - acc: 0.9552 - val_loss: 0.1414 - val_acc: 0.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2674d860>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=True, epochs=16, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to process new sentences so that they can be processed by our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Preprocess a sentence into word vectors for the model\n",
    "    \n",
    "    TODO:\n",
    "        1. split sentence into words (and make lowercase)\n",
    "        2. make each word into a 3-word window (use seq_to_windows)\n",
    "            - this will require the creating some fake tags in the right format\n",
    "                in order to pass to `seq_to_windows`\n",
    "        3. turn each row (3-word window) into a single vector (use window_row_to_vector)\n",
    "        4. turn the list of 150-d vectors into a numpy matrix shape (n_words x 150)\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    # your code here\n",
    "    fake_tags = tuple(random.choice(list(TAG_TO_NUM.keys())) for i in words)\n",
    "    x, _ = seq_to_windows(words=words, tags=fake_tags, tag_to_num=TAG_TO_NUM, word_to_num=word_to_num)\n",
    "    x = window_row_to_vector(window_row=x.T, embed_matrix=embedding_matrix)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    preprocess_sentence('This sentence has five words').shape == \n",
    "    (5, EMBEDDING_DIM * (1 + 2 * WINDOW))\n",
    "), '`preprocess_sentence` does not work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eric      :  per\n",
      "Schmidt   :  per\n",
      "quit      :  o\n",
      "after     :  o\n",
      "Netflix   :  o\n",
      "Inc       :  org\n",
      "announced :  o\n",
      "it        :  o\n",
      "would     :  o\n",
      "acquire   :  o\n",
      "Google    :  org\n",
      "Inc       :  org\n",
      "for       :  o\n",
      "17        :  o\n",
      "dollars   :  o\n",
      ".         :  o\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Come up with a sentence\n",
    "# 2. preprocess it for consumption by the network with `preprocess_sentence`\n",
    "# 3. use the model to make predictions\n",
    "# 4. Turn the predicted probabilities into predicted labels\n",
    "# 5. print the output nicely  (done for you)\n",
    "\n",
    "\n",
    "# Make up some of your own sentences\n",
    "sentence = 'Eric Schmidt quit after Netflix Inc announced it would acquire Google Inc for 17 dollars .'\n",
    "\n",
    "processed_sentence = preprocess_sentence(sentence)\n",
    "predictions = model.predict(processed_sentence)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "maxlen = max(map(len, sentence.split()))\n",
    "for word, label in zip(sentence.split(), predicted_labels):\n",
    "    print('{} :  {}'.format(word.ljust(maxlen), NUM_TO_TAG[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
